[{"path":"index.html","id":"introducción","chapter":"Introducción","heading":"Introducción","text":"Apuntes para las asignatura de Estadística básica y avanzada del Máster en Nuevas Tendencias en Ciencias de la Salud.","code":""},{"path":"exploración-de-los-datos.html","id":"exploración-de-los-datos","chapter":"Capítulo 1 Exploración de los datos","heading":"Capítulo 1 Exploración de los datos","text":"Cuando abordamos el estudio de un conjunto de datos, antes de introducirnos en cuestiones más detalladas, es necesario hacer una exploración inicial de los mismos. Así podemos tener una idea más clara de las características principales de los datos que hemos recogido, y de las posibles asociaciones.En primer lugar, daremos unas ideas sobre la manera de presentar ordenadamente y resumir variables consideradas aisladamente de las demás, para después explorar conjuntamente grupos de variables.","code":""},{"path":"exploración-de-los-datos.html","id":"datos-univariantes","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1 Datos Univariantes","text":"Los métodos para visualizar y resumir los datos dependen de sus tipos, que básicamente diferenciamos en dos: categóricos y numéricos.Los datos categóricos (o factores) son aquellos que registran categorías o cualidades. Si descargamos la base de datos centroSalud-transversal.sav, ejemplos de variables categóricas son el sexo, el estado civil y el nivel de estudios. Dentro de las categóricas podemos su vez distinguir entre variable nominal y ordinal. En esta última hay un orden entre las distintas categorías como se aprecia en la la variable nivel de estudios y tabaqismo:Siguiendo con la misma base de datos de pacientes, si recogemos, el peso de una persona es una cantidad numérica. En particular continua (los valores dentro de cualquier intervalo son posibles); Esto ocurre cuando recogemos el número de hijos; Esta variable es discreta.","code":"\ndf=read_sav(\"datos/centroSalud-transversal.sav\", user_na=FALSE) %>% haven::as_factor()\ndf %>% head() %>% select(sexo:peso) %>% knitr::kable(booktabs=T)"},{"path":"exploración-de-los-datos.html","id":"datos-categóricos","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1.1 Datos categóricos","text":"Los datos categóricos los examinamos bien con tablas de frecuencias o con representaciones gráficas como diagramas de barras o de sectores.","code":""},{"path":"exploración-de-los-datos.html","id":"frecuencias-y-porcentajes","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1.1.1 Frecuencias y porcentajes","text":"Las frecuencias pueden obtenerse en términos absolutos (frecuencias absolutas), mostrando las repeticiones de cada categoría, o bien en términos relativos (porcentajes), mostrando los participación de cada categoría en relación con el total. Las frecuencias absolutas se utilizan con muestras de tamaño pequeño, y las relativas tienen más sentido con muestras de tamaño grande.Si las variables son categóricas ordinales (o numéricas) pueden sernos de interés los porcentajes acumulados. Nos indican para cada valor de la variable, en qué porcentaje de ocasiones se presentó un valor inferior o igual.","code":"\ntabla=KreateTableOne(vars=c(\"sexo\",\"laboral\", \"nivelest\", \"tabaco\",  \"diabm\"),data = df) \ntabla  %>% knitr::kable()"},{"path":"exploración-de-los-datos.html","id":"diagrama-de-barras","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1.1.2 Diagrama de barras","text":"El diagrama de barras se representa asignándole cada modalidad de la variable una barra de una altura proporcional su frecuencia absoluta o su porcentaje. En ambos casos el gráfico es el mismo, sólo se modifica la escala.","code":"\ngrid.arrange(plot_frq(df$sexo),  plot_frq(df$laboral) , plot_frq(df$nivelest), plot_frq(df$diabm),ncol=2)"},{"path":"exploración-de-los-datos.html","id":"diagramas-de-sectores","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1.1.3 Diagramas de sectores","text":"En este diagrama se le asigna cada valor un sector cuyo ángulo sea proporcional su frecuencia. Se suele utilizar en datos categóricos nominales y tanto en los ordinales (es menos clara de interpretar).Si usamos SPSS, tanto tablas de frecuencias como los gráficos mencionados los encontramos en la opción de menú “Analizar – Estadísticos Descriptivos – Frecuencias”.","code":"\ngrid.arrange(\nggplot(df, aes(x = factor(1), fill = sexo)) + geom_bar(width = 1) + coord_polar(theta = \"y\") + theme_void(),\nggplot(df, aes(x = factor(1), fill = nivelest)) + geom_bar(width = 1) + coord_polar(theta = \"y\") + theme_void(),\nnrow=1)"},{"path":"exploración-de-los-datos.html","id":"datos-numéricos.","chapter":"Capítulo 1 Exploración de los datos","heading":"1.1.2 Datos Numéricos.","text":"Los datos numéricos son mucho más ricos en información que los datos categóricos. Por tanto además de las tablas, tenemos otras medidas que sirven para resumir la información que contienen. Dependiendo de cómo se distribuyan los datos, usaremos grupos de medidas de resumen diferentes.Cuando se tiene una variable numérica, lo primero que nos puede interesar es alrededor de qué valor se agrupan los datos, y cómo se dispersan con respecto él.En múltiples ocasiones los datos presentan cierta distribución acampanada como la de la figura adjunta, denominada distribución normal. En estos casos con sólo dos medidas como son la media y la desviación típica tenemos resumida prácticamente toda la información contenida en las observaciones.La media: es el promedio de todos los valores de la variable, es decir, la suma de todos los datos dividido por el número de ellos. La desviación típica (S) nos da una medida de la dispersión que tienen los datos con respecto la media. En datos de distribución acampanada (aproximadamente normal), ocurre lo siguiente:Entre la media y una distancia de una desviación típica se encuentra (aproximadamente) el 68% central de los datos.Entre la media y una distancia de una desviación típica se encuentra (aproximadamente) el 68% central de los datos.Entre la media y una distancia de dos desviación típica se encuentra (aproximadamente) el 95% central de los datos.Entre la media y una distancia de dos desviación típica se encuentra (aproximadamente) el 95% central de los datos.Sigue este enlace para practicar con la desviación típicaLa media y la desviación muestral tienen tanto interés cuando los datos presentan largas colas u observaciones anómalas (outliers), es decir, son muy influenciables por las asimetrías y los valores extremos. En estos casos, debemos considerar medidas más resistentes estas influencias.Como medidas de centralización resistentes podemos utilizar en sustitución de la media:La mediana, que es aquel valor que deja la mitad de los datos por debajo de él.La mediana, que es aquel valor que deja la mitad de los datos por debajo de él.La media recortada (trimmed mean), muy utilizada en datos preferentemente simétricos, con muchas observaciones anómalas y, que se obtiene eliminando un determinado porcentaje de los datos menores y mayores; Así calculamos la media sin contar con ese porcentaje de datos extremos, haciendo desaparecer su influencia.La media recortada (trimmed mean), muy utilizada en datos preferentemente simétricos, con muchas observaciones anómalas y, que se obtiene eliminando un determinado porcentaje de los datos menores y mayores; Así calculamos la media sin contar con ese porcentaje de datos extremos, haciendo desaparecer su influencia.En cuanto las medidas de dispersión más resistentes podemos utilizar el rango intercuartílico (IQR), que es la diferencia entre el tercer cuartil y el primero. El primer cuartil (Q1) deja al 25% de los datos por debajo de él y el tercer cuartil (Q3) deja al 75%, por tanto sabemos que entre ambos valores se encuentra el 50% central de las observaciones.Para practicar con percentiles sigue este enlaceAhora bien, ¿qué criterios aproximados podemos utilizar para clasificar unos datos como normales o ? Para ello destacamos varias características de la distribución normal. El alejamiento de las mismas es indicación de falta de normalidad:Es simétrica (el coeficiente de asimetría vale cero)Tiene forma de campana (el apuntamiento o curtosis vale cero).Coinciden la media y la medianaAproximadamente el 95% de las observaciones se encuentran en el intervalo de centro la media y radio dos veces la desviación típica.Los indicadores que miden la simetría y la forma de la campana son el coeficiente de asimetría (skewness) (negativo en distribuciones con cola la izquierda, positivo en distribuciones con cola la derecha) y la curtosis (kurtosis) (negativa para las aplanadas y positiva para las apuntadas).En la tablas anteriores, así como en los gráficos (llamados histogramas) vemos como peso e IMC presentan una cierta falta de normalidad; Podríamos entonces presentar un resumen de estas variables del siguiente modo:La falta de normalidad es fácil de apreciarlo mirando directamente el histograma. Hay gráficos como el Q-Q plot, que nos indican la falta de normalidad como desviaciones de la observaciones con respecto una línea recta:Las medida mencionadas podemos calcularlas con SPSS en el menú “Analizar – Estadísticos Descriptivos – Frecuencias” y pulsando el botón “Estadísticos…”, o bien podemos usar el menú “Analizar - Estadísticos descriptivos - Explorar”, donde podemos añadir los gráficos con pruebas de normalidad.","code":"\nplot_frq(df$talla, type = \"hist\", show.mean = TRUE,normal.curve = TRUE)\ngrid.arrange(\nplot_frq(df$peso, type = \"hist\", show.mean = TRUE,normal.curve = TRUE)+coord_cartesian(xlim=c(40,160)),\nplot_frq(df$peso, type = \"boxplot\", show.mean = TRUE,normal.curve = TRUE) + \n  coord_cartesian(ylim=c(40,160))+ coord_flip(), nrow=2)\ngeneraTablaDescriptivaNumericas(df,c(\"edad\",\"peso\",\"talla\",\"imc\"),\n               columnas = c(\"n\", \"media\",\"dt\",\"min\",\"p25\",\"p50\",\"p75\",\"max\",\"asim\",\"curtosis\")) %>%\n  kable(booktabs=T)\ngrid.arrange(\nggplot(df,aes(x=edad))+geom_histogram(),\nggplot(df,aes(x=talla))+geom_histogram(),\nggplot(df,aes(x=peso))+geom_histogram(),\nggplot(df,aes(x=imc))+geom_histogram(),\nnrow=2)\ntabla=KreateTableOne(data = df %>% select(edad,talla,peso,imc),nonnormal=c(\"peso\",\"imc\"))\ntabla %>% knitr::kable(booktabs=TRUE)\ngrid.arrange(\nggplot(df, aes(sample =edad))  +  stat_qq() + stat_qq_line()+ggtitle(\"edad\"),\nggplot(df, aes(sample =talla))  +  stat_qq() + stat_qq_line()+ggtitle(\"peso\"),\nggplot(df, aes(sample =peso)) + stat_qq() +  stat_qq_line()+ggtitle(\"talla\"),\nggplot(df, aes(sample =imc))   +  stat_qq() + stat_qq_line()+ggtitle(\"imc\"), \nnrow=2)"},{"path":"exploración-de-los-datos.html","id":"datos-bivariantes","chapter":"Capítulo 1 Exploración de los datos","heading":"1.2 Datos bivariantes","text":"Si resumir la información de una variable es de por sí interesante, en investigación lo es mucho más el poner de manifiesto la posible relación entre dos de ellas:¿Hay relación entre el tabaco y el cáncer de pulmón? ¿Aumentando la dosis de un medicamento, mejoramos la respuesta?Para ello realizamos estudios donde intervienen ambas variables simultáneamente. Según sean los tipos de cada una de ellas usaremos técnicas diferentes.","code":""},{"path":"exploración-de-los-datos.html","id":"categórica-categórica","chapter":"Capítulo 1 Exploración de los datos","heading":"1.2.1 Categórica-categórica","text":"Cuando ambas variables son categóricas (o discretas con pocas modalidades), se suele presentar las observaciones en una tabla de contingencia. Esta una tabla de doble entrada donde se presentan la distribución de frecuencias conjunta de las dos variables.Continuando con la base de datos del ejemplo, podríamos estudiar qué distribución presentan otras variables cualitativas según el sexo del paciente. Lo mostraríamos como sigue:En la tabla anterior hay una columna denominada p (significación) que será importante en temas posteriores.En cuanto la representación gráfica, podemos utilizar el diagrama de barras apiladas o agrupadas, aunque en ellos es inmediato apreciar las diferencias por sexosDesglosando en cada categoría de la variable los porcentajes de cada sexo es más sencillo de ver:","code":"\ntabla=KreateTableOne(vars = c(\"tabaco\",\"estcivil\",\"sedentar\",\"diabm\",\"hipercol\"), strata = \"sexo\" , data = df)\ntabla %>% knitr::kable(booktabs=T)\ngrid.arrange(\nplot_grpfrq(df$tabaco, df$sexo,show.prc = FALSE),\nplot_grpfrq(df$estcivil, df$sexo,,show.prc = FALSE),\nplot_grpfrq(df$sedentar, df$sexo,show.prc = FALSE),\nplot_grpfrq(df$diabm, df$sexo,show.prc = FALSE),\nplot_grpfrq(df$hipercol, df$sexo,show.prc = FALSE),ncol=1)\ndftmp=df %>% select(sexo,tabaco)%>% filter(!is.na(tabaco)) %>% \n  mutate(cuenta=1) %>% group_by(sexo,tabaco) %>% \n  tally() %>% mutate(fraccion=n/sum(n))\nggplot(dftmp, aes(fill=sexo, y=fraccion, x=tabaco)) + \n    geom_bar( stat=\"identity\", position=\"fill\")\ndftmp=df %>% select(sexo,diabm) %>% filter(!is.na(diabm)) %>% mutate(cuenta=1) %>% group_by(sexo,diabm) %>% tally() %>% mutate(fraccion=n/sum(n))\nggplot(dftmp, aes(fill=sexo, y=fraccion, x=diabm)) + \n    geom_bar( stat=\"identity\", position=\"fill\")"},{"path":"exploración-de-los-datos.html","id":"categórica-numérica","chapter":"Capítulo 1 Exploración de los datos","heading":"1.2.2 Categórica-Numérica","text":"Supongamos que tenemos datos numéricos para varias categorías. Por ejemplo, en un experimento donde hacemos mediciones numéricas en dos grupos: uno al que se le aplica determinado tratamiento y otro de control. Podemos describir los resultados del experimento con sólo dos variables: Una variable categórica que representa el grupo de tratamiento, y otra que representa el resultado numéricoEn estos casos, lo que se realiza es un estudio descriptivo de la variable numérica en cada una de las muestras y comparamos los resultados.Volviendo nuestro ejemplo, vamos comparar las variables numéricas de la base de datos entre sexos:En las tablas anteriores aparecen de nuevos las cantidades p (significación) de las que hablaremos más adelante.Los diagramas de cajas muestran los cuartiles en unas cajas centrales, así como observaciones más alejadas, y permiten hacerse una idea visual de qué diferencias existen entre los grupos. Por ejemplo, en las tablas anteriores se apreciaba una cierta diferencia de talla entre hombres y mujeres, aunque así en pad:Si usamos SPSS, tenemos nuestra disposición la opción de menú “Analizar – Estadísticos descriptivos – Explorar…”. En la casilla denominada “dependientes” situamos la variable numérica y en “factores” situamos la categórica.","code":"\ntabla=KreateTableOne(vars = c(\"edad\", \"talla\",\"peso\",\"imc\"), strata = \"sexo\" , data = df) \ntabla %>% knitr::kable(booktabs=T)\ntabla=KreateTableOne(data=df,vars = c(\"edad\",\"peso\",\"talla\",\"imc\",\"pas\",\"pad\",\"fc\"), strata = \"tabaco\" ) \ntabla %>% knitr::kable(booktabs=T)\ngrid.arrange(\nggplot(df,aes(x=sexo,y=talla))+geom_boxplot(),\nggplot(df,aes(x=sexo,y=pad))+geom_boxplot(),ncol=2)"},{"path":"exploración-de-los-datos.html","id":"numérica-numérica.","chapter":"Capítulo 1 Exploración de los datos","heading":"1.2.3 Numérica-Numérica.","text":"Cuando hablamos de comparar dos variables numéricas, pensamos en establecer la posible relación entre ellas.¿Estarán relacionados la altura y el peso de los individuos? ¿Cuanto mayor es el tamaño del cerebro, mayor es el coeficiente intelectual?La vía más directa para estudiar la posible asociación consiste en inspeccionar visualmente un diagrama de dispersión (nube de puntos). Si reconocemos una tendencia, es una indicación de que puede valer la pena explorar con más profundidad. Si es el caso, puede interesarnos proseguir con un análisis de regresión. En este tipo de análisis se pretende encontrar un modelo matemático (recta de regresión) que explique los valores de una de las variables (dependiente) en función de la otra (independiente). ello le dedicamos un capítulo con posterioridad.Por ejemplo, en la base de datos con que trabajamos, es lógico esperar una buena relación entre el peso y el imc, y eso es justo lo que encontramos.En otras variables la relación es tan evidente, como la que existe entre edad y mayor presión arterial diastólica y menor presión arterial sistólica. Las rectas de regresión serán muy útiles para percibir la tendencia con más facilidad (aunque esto solo lo utilizamos ahora como ayuda visual).Para describir numéricamente el grado de asociación lineal dentre variables numéricas suele usarse el coeficiente de correlación lineal de Pearson (r). Esta es una cantidad adimensional que toma valores entre -1 y 1. Cuando r=0 se dice que hay incorrelación (nada de asociación lineal). Cuánto más se aleje r de cero, mayor es el grado de asociación lineal entre las vaiables.Para practicar con el coeficiente de correlación lineal de Pearson, siga este enlaceLos gráficos mostrados se realizan en SPSS en el menú: “Gráficos – Dispersión… – Simple“. El coeficiente de correlacion lineal de Pearson lo encontramos en el menú:”Analizar - Correlaciones - Bivariadas”.Si queremos mostrar todas las correlaciones existentes entre las variables numéricas de nuestra base de datos, tendremos un r que mostrar por cada par de variables. Eso hace una buena cantidad de números. Una forma habitual de mostrarlos sin ocupar mucho espacio es esta:","code":"\nggplot(df, aes(x=peso, y=imc)) + geom_jitter(alpha=0.3)\ngrid.arrange(\n  ggplot(df, aes(x=edad, y=pas)) + geom_jitter(alpha=0.3)+geom_smooth(method=\"lm\"),\nggplot(df, aes(x=edad, y=pad)) + geom_jitter(alpha=0.3)+geom_smooth(method=\"lm\"),nrow=1)\ndf %>% generaTablaCorrelaciones(vNumericas = c(\"edad\",\"peso\",\"talla\",\"imc\",\"pas\",\"pad\",\"fc\")) %>%\n  knitr::kable(booktabs=T)"},{"path":"estimación.html","id":"estimación","chapter":"Capítulo 2 Estimación","heading":"Capítulo 2 Estimación","text":"En esta capítulo trataremos sobre como usar una muestra para obtener información sobre la población de la que se ha extraído. Esto nos conducirá introducir los conceptos de intervalo de confianza y de error típico de estimación.","code":""},{"path":"estimación.html","id":"intervalos-de-confianza","chapter":"Capítulo 2 Estimación","heading":"2.1 Intervalos de confianza","text":"Un problema habitual es el de estimar parámetros que ayuden caracterizar una variable. Por ejemplo el porcentaje de individuos que mejora ante un cierto tratamiento, o el tiempo medio que tarda un anestésico en hacer efecto.Podríamos decir, tras realizar un estudio, “el 75% de los pacientes tratados experimentó una mejoría”. Una respuesta más sofisticada usando intervalos de confianza podría ser: “Nuestro estudio muestra que en el 75% de los casos se experimenta una mejoría, siendo el margen de error del 6%. El nivel de confianza es del 95%”.El cálculo de intervalos de confianza para la estimación de parámetros son técnicas que nos permiten hacer declaraciones sobre qué valores podemos esperar para un parámetro. El intervalo calculado dependerá de:Lo estimado en la muestra (porcentaje, media,…) El intervalo de confianza esta formado por valores ligeramente menores y mayores que la aproximación ofrecida por la muestra.Lo estimado en la muestra (porcentaje, media,…) El intervalo de confianza esta formado por valores ligeramente menores y mayores que la aproximación ofrecida por la muestra.El tamaño muestral. Cuantos más datos hayan participado en el cálculo, más pequeño esperamos que sea la diferencia entre el valor estimado y el valor real desconocido.El tamaño muestral. Cuantos más datos hayan participado en el cálculo, más pequeño esperamos que sea la diferencia entre el valor estimado y el valor real desconocido.La probabilidad (nivel de confianza) con la que el método dará una respuesta correcta. Un nivel de confianza habitual para los intervalos de confianza es el 95%.La probabilidad (nivel de confianza) con la que el método dará una respuesta correcta. Un nivel de confianza habitual para los intervalos de confianza es el 95%.Puede parecer sorprendente que busquemos respuestas con una confianza del 100%, pero ocurre que en ese caso, los intervalos serían tan grandes que serían de gran provecho. La elección de un nivel de confianza como el 95% es un compromiso entre hacer declaraciones con una razonable probabilidad de acertar, y que además el intervalo declarado, sea lo suficientemente pequeño como para suscitar algún interés. El nivel de confianza hay que interpretarlo como que disponemos de un método de calcular intervalos que seguido con rigor, en cierto porcentaje de casos acierta (nivel de confianza) y en el resto falla.","code":""},{"path":"estimación.html","id":"error-típico-o-estándar","chapter":"Capítulo 2 Estimación","heading":"2.2 Error típico o estándar","text":"En multitud de ocasiones al utilizar un programa estadístico encontramos junto las más diversas estimaciones como una media, una proporción, un coeficiente de regresión, un coeficiente de asimetría, etc., una cantidad denominada error estándar o también error típico.El error estándar tiene mucho que ver con los intervalos de confianza. Para muchos parámetros, su intervalo de confianza es habitualmente la estimación obtenida sobre la muestra (proporción, media,…), y un margen de error que nos es más que un múltiplo del error estándar. Un ejemplo muy común, consiste en elegir niveles de confianza del 95%. Para ello un margen de error de dos errores estándar es habitualmente la respuesta.Cuando un programa estadístico nos ofrece una estimación de una cantidad, junto su error estándar, podemos estar seguros de que si se dan ciertas condiciones de validez, el estimador del parámetro tiene comportamiento normal cuya desviación típica es el error estándar, y por tanto una declaración como:“La proporción de pacientes que mejoraron con el tratamiento es del 0.75, con un error estándar del 0.03”puede enunciarse de forma más clara en forma de intervalo de confianza:La proporción de pacientes que mejoró fue del 0.75, siendo el intervalo de confianza al 95% 0.69—0.81 .Es fácil confundir desviación típica con error típico. Ambos hablan de la dispersión en torno un valor central y se usan en distribuciones aproximadamente normales. Peo el segundo se utiliza solo en el contexto de estimar un valor partir de una muestra.Practique con la diferencia entre desviación típica y error típico","code":""},{"path":"estimación.html","id":"ejemplo-de-intervalos-de-confianza-y-errores-estándar","chapter":"Capítulo 2 Estimación","heading":"Ejemplo de intervalos de confianza y errores estándar","text":"Se consideran dos grupos de individuos que desmpeñan un trabajo similar, pero que siguen una forma de alimentación muy diferente. Están formados por albañiles de Málaga y Tánger. Descargue la base de datos 2poblaciones-Mismotrabajo-DiferenteNutricion.sav. Las primeras líneas tienen el siguiente aspecto:Si usa SPSS en el menú “Analizar - Estadística discriptiva - Explorar”, colocando las variables numéricas en el campo Dependientes y el Grupo en el campo Factor puede obtener información para construir estos resultados:Visualmente se aprecia que los individuos de Grupo==Málaga siendo de Talla similar, presentan mayor Consumo de energía y similar Gasto. Se observa en ellos mayores valores de Colesterol, Trigliceridos, Glucemia y Peso, y valores similares de Presión arterial.Los intervalos de confianza y errores estándar podrían mostrarse en una tabla como sigue:En la tabla anterior vemos para cada grupo solo la media y desviación típica de cada grupo (que nos da una idea de donde se sitúan los individuos de cada grupo), sino un la precisión con la que se ha estimado la media de cada grupo (en forma de error estándar e intervalo de confianza al 95%).Realmente es esta la forma habitual de presentar los resultados en una publicación científica. Normalmente queremos mostrar como es la precisión de nuestras estimaciones, y el intervalo de confianza se muestra más bien para la diferencia que hay entre los dos grupos. Hay que esperar ver la prueba t-student para ver como construir la tabla habitual:Las barras ocupan mucho espacio y permiten apreciar bien las diferencias. Personalmente pienso que es mejor mostrar solo los intervalos formados por las medias y el error típico como sigue:","code":"\ndf=read_sav(\"datos/2poblaciones-Mismotrabajo-DiferenteNutricion.sav\", user_na=FALSE) %>% haven::as_factor() %>% mutate(Talla=Talla*100)\ndf %>% head()  %>% knitr::kable(booktabs=T)\nvNumericas=names(df) %>% setdiff(\"Grupo\")\nlistaGraficos=vNumericas %>% map( ~ ggplot(df %>% mutate(Respuesta=df[[.x]]) ,aes(x=Grupo,y=Respuesta))+geom_boxplot(fill=\"lightblue\")+xlab(.x)+ylab(\"\"))\ndo.call(\"grid.arrange\", c(listaGraficos, ncol=3))\nvNumericas=names(df) %>% setdiff(\"Grupo\")\ndf %>% generaTablatTestPorGrupo(\"Grupo\", vNumericas,\n                                columnas = c(\"media\",\"dt\",\"et\",\"ic1\",\"ic2\")) %>% \n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                        \"media\",\"dt\",\"et\",\"ic(min)\",\"ic(max)\",\n                        \"media\",\"dt\",\"et\",\"ic(min)\",\"ic(max)\")) %>%\n  add_header_above(c(\" \" = 1, \"Tanger\" = 5, \"Málaga\" = 5)) %>%\n  kable_styling(font_size = 9)\n\ndf %>% generaTablatTestPorGrupo(\"Grupo\", vNumericas,\n                                columnas = c(\"n\",\"mediaet\",\"p.t\",\"ci95\")) %>% \n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                        \"n\",\"media±et\", \n                        \"n\",\"media±et\",\n                        \"p dif.\",\"ic95% dif.\")) %>%\n  add_header_above(c(\" \" = 1, \"Tanger\" = 2, \"Málaga\" = 2,\" \"=2))\nresumen=df %>% gather(Variable,Valor,-Grupo) %>% \n  group_by(Grupo,Variable) %>% \n  summarise(Media=mean(Valor),\n                n=length(Valor),\n               ET=sd(Valor)/sqrt(n),\n               IC=ET*qt(0.975,n-1))\n\nlistaGraficos=vNumericas %>% map( ~ ggplot(resumen %>% filter(Variable==.x), aes(x=Grupo,y=Media)) +\n  geom_errorbar(aes(ymin=Media-ET,ymax=Media+ET),width=0.2, size=1, color=\"navyblue\")+\n  geom_bar(stat=\"identity\", fill=\"lightblue\", alpha=0.5)+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(.x))\ndo.call(\"grid.arrange\", c(listaGraficos, ncol=3))\nlistaGraficos=vNumericas %>% map( ~ ggplot(resumen %>% filter(Variable==.x), aes(x=Grupo,y=Media)) +\n  geom_errorbar(aes(ymin=Media-ET,ymax=Media+ET),width=0.2, size=1, color=\"navyblue\")+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(.x))\ndo.call(\"grid.arrange\", c(listaGraficos, ncol=3))"},{"path":"contrastes-de-hipótesis.html","id":"contrastes-de-hipótesis","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"Capítulo 3 Contrastes de hipótesis","text":"Aunque gran parte de la investigación médica esta relacionada con la recogida de datos con propósito descriptivo, otra buena parte lo está con la recolección de información con el plan de responder cuestiones puntuales, es decir, contrastes de hipótesis.Las técnicas de contrastes de hipótesis están muy vinculadas las de cálculo de intervalos de confianza, como mencionábamos al hablar de estos últimos. Aunque la aproximación es diferente:Al hacer un intervalo de confianza establecemos una región donde esperamos que esté el valor del parámetro.Al hacer un intervalo de confianza establecemos una región donde esperamos que esté el valor del parámetro.Al hacer un contraste de hipótesis establecemos posibles valores para unos parámetros y calculamos la probabilidad de que se obtengan muestras tan discrepantes o más que la obtenida, bajo la suposición de que la hipótesis es cierta. Si dicha probabilidad es muy baja (por debajo de una cantidad denominada nivel de significación) la hipótesis es rechazada.Al hacer un contraste de hipótesis establecemos posibles valores para unos parámetros y calculamos la probabilidad de que se obtengan muestras tan discrepantes o más que la obtenida, bajo la suposición de que la hipótesis es cierta. Si dicha probabilidad es muy baja (por debajo de una cantidad denominada nivel de significación) la hipótesis es rechazada.Vamos ilustrarlo retomando el ejemplo del tema anterior de intervalos de confianza. Descargue la base de datos 2poblaciones-Mismotrabajo-DiferenteNutricion.sav y observemos las primeras líneas:Exploremos los intervalos de confianza de las medias de cada grupo. En SPSS recordamos que se obtenían en el menú “Analizar - Estadística discriptiva - Explorar”, colocando las variables numéricas en el campo Dependientes y el Grupo en el campo Factor. Los valores que nos interesan son:Si pretendemos contrastar la hipótesis (muy interesante) de si el Colesterol medio en la población de la que se han obtenidos las muestras en Tánger es 190, la respuesta que elegiríamos es , dado que 190 forma parte del intervalo dende casi seguro esperamos encontrar la media. Los datos indican que la media allí es inferior.Análogamente, en Málaga podríamos decir que la media de la población casi seguro que es 190. Los datos apuntan que la media es superior.En término de contrastes de hipótesis diríamos que tenemos evidencia estadísticamente significativa de que tanto en Málaga como en Tanger la media de colesterol de las poblaciones donde se han extraído las muestras es 190, con un nivel de significación del 5%. Este nivel de significación hace el papel de probabilidad de observar unas muestras tan extañas como las observadas si la media fuese 190.","code":"\ndf=read_sav(\"datos/2poblaciones-Mismotrabajo-DiferenteNutricion.sav\", user_na=FALSE) %>% haven::as_factor()\ndf %>% head()  %>% knitr::kable(booktabs=T)\nvNumericas=names(df) %>% setdiff(\"Grupo\")\ndf %>% generaTablatTestPorGrupo(\"Grupo\", vNumericas,\n                                columnas = c(\"ic1\",\"ic2\")) %>% \n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                        \"ic(min)\",\"ic(max)\",\n                        \"ic(min)\",\"ic(max)\")) %>%\n  add_header_above(c(\" \" = 1, \"Tanger\" = 2, \"Málaga\" = 2))\nresumen=df %>% gather(Variable,Valor,-Grupo) %>% \n  group_by(Grupo,Variable) %>% \n  summarise(Media=mean(Valor),\n                n=length(Valor),\n               IC=sd(Valor)/sqrt(n)*qt(0.975,n-1))\nggplot(resumen %>% filter(Variable==\"Colesterol\"), aes(x=Grupo,y=Media)) +\n  geom_errorbar(aes(ymin=Media-IC,ymax=Media+IC),width=0.2, size=1, color=\"navyblue\")+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(\"Colesterol\")+\n  geom_hline(yintercept=190,lty=2)"},{"path":"contrastes-de-hipótesis.html","id":"qué-es-una-hipótesis","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.1 ¿Qué es una hipótesis?","text":"Las hipótesis que se contrastan hay que entenderlas como una declaración, como una pregunta responder. se formulan como la cuestión “¿El tratamiento tiene efecto?”. Se formulan bien como “El tratamiento tiene efecto” o como “el tratamiento sí tiene efecto”.una hipótesis que traduce las ideas de “hay efecto”, “hay relación”, “los resultados en los grupos son similares”, se la denomina hipótesis nula.una hipótesis que traduce las ideas de “hay efecto”, “hay relación”, “los resultados en los grupos son similares”, se la denomina hipótesis nula.La hipótesis alternativa es la que se aceptará cuando los datos nos inviten rechazar la nula.La hipótesis alternativa es la que se aceptará cuando los datos nos inviten rechazar la nula.Las hipótesis se formularán normalmente como una declaración sobre una o más poblaciones, especialmente sobre sus parámetros. Ejemplos de hipótesis pueden ser:La edad media de los individuos con la enfermedad X es 50 años.La edad media de los individuos con la enfermedad X es 50 años.El tratamiento X tiene el mismo efecto que un placebo (por tanto tiene valor terapéutico). Se podría traducir como que el efecto medio del placebo es igual al efecto medio del tratamiento.El tratamiento X tiene el mismo efecto que un placebo (por tanto tiene valor terapéutico). Se podría traducir como que el efecto medio del placebo es igual al efecto medio del tratamiento.Para determinar la cantidad de anestésico aplicar un paciente, se utiliza un modelo de regresión lineal, siendo el peso y la edad del mismo variables independientes. Alguna hipótesis podría ser que deberíamos tener en cuenta la edad, por ejemplo. Esto se traduciría en que el coeficiente asociado en el modelo de regresión es cero.Para determinar la cantidad de anestésico aplicar un paciente, se utiliza un modelo de regresión lineal, siendo el peso y la edad del mismo variables independientes. Alguna hipótesis podría ser que deberíamos tener en cuenta la edad, por ejemplo. Esto se traduciría en que el coeficiente asociado en el modelo de regresión es cero.Al tratar de identificar factores de riesgo para el desarrollo de una enfermedad podríamos estar interesados en saber si fumar es uno de ellos. Para ello podríamos realizar una regresión logística y contrastar si la “odds ratio” es igual uno, lo que equivale decir que es un factor de riesgo.Al tratar de identificar factores de riesgo para el desarrollo de una enfermedad podríamos estar interesados en saber si fumar es uno de ellos. Para ello podríamos realizar una regresión logística y contrastar si la “odds ratio” es igual uno, lo que equivale decir que es un factor de riesgo.En todo contraste de hipótesis se enfrenta la denominada hipótesis nula (“efecto”), frente la hipótesis alternativa, que la niega. La hipótesis nula puede interpretarse como aquella que normalmente sería aceptada mientras los datos indiquen otra cosa. Cuando los datos se muestran contrarios la hipótesis nula la vez que favorables la hipótesis alternativa, se rechaza la nula y se acepta la alternativa. El mecanismo puede ser más simple, pero hay que tener cuidado con interpretarlo bien, pues pueden esconderse trampas de emplearse con atención.Un contraste se declara como estadísticamente significativo, cuando partir de los resultados muestrales concluimos que se rechaza la hipótesis nula.En los capítulos siguientes trataremos contrastes de hipótesis de frecuente uso en la ciencia médica, indicando en cada caso la hipótesis nula y la alternativa.","code":""},{"path":"contrastes-de-hipótesis.html","id":"tipos-de-error","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.2 Tipos de error","text":"Al realizar un contraste de hipótesis hay dos tipos de errores posibles:Rechazar la hipótesis nula, cuando esta es cierta. esto se le denomina error de tipo .Rechazar la hipótesis nula, cuando esta es cierta. esto se le denomina error de tipo .rechazar la hipótesis nula, cuando esta es falsa. Lo denominamos error de tipo II, y la probabilidad de que ocurra se la denomina β.rechazar la hipótesis nula, cuando esta es falsa. Lo denominamos error de tipo II, y la probabilidad de que ocurra se la denomina β.","code":""},{"path":"contrastes-de-hipótesis.html","id":"nivel-de-significación","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.3 Nivel de significación","text":"Se denomina nivel de significación de un contraste, la probabilidad de cometer un error de tipo . La probabilidad de que este evento ocurra, es una cantidad que se fija de antemano (antes incluso de extraer las muestras) en un número pequeño denominado nivel de significación, y se representa con la letra α. Típicamente se elige un valor pequeño, 5% o 1%. Todo experimento, en su definición y antes de elegir las muestras, debe llevar descrito cuál es el criterio con el que rechazaremos una hipótesis. Esto se traduce en prefijar el nivel de significación del contraste.Al leer un artículo, en la sección “material y métodos”, es frecuente encontrárselo referenciado en frases del estilo:“Se declararán significativos los contrastes cuando la significación sea inferior al 5%”.Esta frase debe entenderse como que se rechazará la hipótesis nula del contraste si, al examinar la muestra, se observa que discrepa tanto de la hipótesis nula que, si esta fuese realmente cierta, la probabilidad de obtener una muestra como la obtenida (o aún peor), es inferior al 5%. Dicho con otros términos,“Declaramos que hay efecto, hasta que los datos nos indique otra cosa más allá de una duda razonable”.","code":""},{"path":"contrastes-de-hipótesis.html","id":"significación","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.4 Significación","text":"Se denomina significación de un contraste, y se suele representar con la letra p, al valor calculado tras observar la muestra, y que debería tomar el nivel de significación para estar en una situación donde dudemos entre rechazar o una hipótesis.Visto así, podemos considerar la significación como un indicador de la discrepancia entre una hipótesis nula y los datos muestrales. Cuanto más cercano sea cero, más evidencia tenemos en contra de la hipótesis nula; Hasta el punto que cuando la significación es inferior al nivel de significación fijado de antemano (por ejemplo el 5%), directamente se rechaza la hipótesis nula (se ha superado el umbral de los que se había fijado de antemano como una “duda razonable favor de que hay efecto”)","code":""},{"path":"contrastes-de-hipótesis.html","id":"potencia","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.5 Potencia","text":"La probabilidad de que ocurra el error de tipo II, cuando esta puede calcularse, se la denomina potencia del contraste. Es decir, la potencia del contraste es una medida de la habilidad de un contraste para detectar un efecto que está presente.En los casos en que es posible fijar la potencia del contraste suele tomarse valores como 85% o 90%. Aparece normalmente en contrastes donde la hipótesis alternativa afirma que solo un tratamiento es mejor que un placebo, sino que además es mejor en, al menos, cierta cantidad que se considera clínicamente significativa (confundir con estadísticamente significativa).Normalmente los programas estadísticos solamente hacen referencia la significación de un contraste, y la potencia. Ello se debe que para determinar la potencia es necesario que la hipótesis alternativa sea simplemente una negación de la hipótesis nula, sino que debe especificar explícitamente una diferencia cuantitativa con ella. Esta diferencia es algo que depende del investigador, de lo los datos.","code":""},{"path":"contrastes-de-hipótesis.html","id":"ejemplo-simple","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"Ejemplo simple","text":"Se espera que un tratamiento produzca un efecto beneficioso en la reducción del colesterol de al menos 10 unidades. Para ello se eligen dos grupos de pacientes de características similares y se prueba con un grupo un placebo y con el otro el tratamiento en cuestión.Mientras los datos indiquen lo contrario pensaremos que el tratamiento tiene efecto (consigue reducir de media las 10 unidades). Esta es la hipótesis nula.Mientras los datos indiquen lo contrario pensaremos que el tratamiento tiene efecto (consigue reducir de media las 10 unidades). Esta es la hipótesis nula.La hipótesis alternativa es que el tratamiento sí tiene efecto (bajará el colesterol esas 10 unidades, o más).La hipótesis alternativa es que el tratamiento sí tiene efecto (bajará el colesterol esas 10 unidades, o más).Un nivel de significación del 5% se interpreta como que menos que los resultados del experimento discrepen suficientemente de que el tratamiento tiene efecto, consideraremos que el tratamiento sirve. Si el tratamiento muestra resultados muy buenos con respecto al placebo decidiremos que sí sirve, y rechazaremos la hipótesis nula. Por supuesto, excepcionalmente puede ocurrir que un tratamiento que tiene ninguna utilidad muestre resultados buenos sobre una muestra, y acabemos concluyendo por error que es bueno. La probabilidad de que esto ocurra evidentemente esperamos que sea baja y es lo que hemos denominado nivel de significación.Un nivel de significación del 5% se interpreta como que menos que los resultados del experimento discrepen suficientemente de que el tratamiento tiene efecto, consideraremos que el tratamiento sirve. Si el tratamiento muestra resultados muy buenos con respecto al placebo decidiremos que sí sirve, y rechazaremos la hipótesis nula. Por supuesto, excepcionalmente puede ocurrir que un tratamiento que tiene ninguna utilidad muestre resultados buenos sobre una muestra, y acabemos concluyendo por error que es bueno. La probabilidad de que esto ocurra evidentemente esperamos que sea baja y es lo que hemos denominado nivel de significación.Supongamos que tras estudiar los resultados del experimento con algún contraste obtenemos que la significación es p=0.001=0.1%. Esto quiere decir que los resultados de la muestra son incoherentes con la hipótesis nula (son más improbables que el valor límite que fue fijado en 5%). Concluimos que hay evidencia estadísticamente significativa en contra de que el tratamiento reduce el colesterol en al menos 10 unidades.Supongamos que tras estudiar los resultados del experimento con algún contraste obtenemos que la significación es p=0.001=0.1%. Esto quiere decir que los resultados de la muestra son incoherentes con la hipótesis nula (son más improbables que el valor límite que fue fijado en 5%). Concluimos que hay evidencia estadísticamente significativa en contra de que el tratamiento reduce el colesterol en al menos 10 unidades.¿Cómo se interpreta la potencia en este ejemplo? Supongamos que el tratamiento realmente es efectivo (reduce al menos 10 unidades el colesterol). ¿Cuál sería la probabilidad de que una prueba estadística lo detecte (rechace la hipótesis nula)? Esta probabilidad sería la potencia.¿Cómo se interpreta la potencia en este ejemplo? Supongamos que el tratamiento realmente es efectivo (reduce al menos 10 unidades el colesterol). ¿Cuál sería la probabilidad de que una prueba estadística lo detecte (rechace la hipótesis nula)? Esta probabilidad sería la potencia.Para practicar con estos conceptos puede trabajar con esta simulación","code":""},{"path":"contrastes-de-hipótesis.html","id":"ejemplo-real-lectura-de-un-artículo","chapter":"Capítulo 3 Contrastes de hipótesis","heading":"3.5.1 Ejemplo real: Lectura de un artículo","text":"Siga este enlace para consultar como se ha redactado la publicación de un artículo científico concreto. Vamos destacar de él agunos pasajes y vamos traducir algunas frases los conceptos anteriores:measured biventricular strains 1824 randomly selected patients (973 men, aged 70±14 years) strain registry.En primer lugar, 70±14 nos indica que la media es 70 años. todos tenían la misma edad, 14 se utiliza en este contecto como medida de dispersión, y es de esperar que la edad se separase mucho de una distribución normal de desviación típica 14 años. Seguramente había muchos individuos que se alejasen 2*14=28 años de la media.total 799 patients (43.8%) died median follow‐duration 31.7 months.Cada paciente fue seguido una cantidad de tiempo diferente. La mediana o percentil 50, fue de 31.7 meses, es decir, la mitad de ellos fue seguido un tiempo inferior o igual. El resto fue medido un tiempo superior. La evolución de los pacientes es tal que esos 31.7 meses ya habían fallecidol 43.8% de ellos.univariate analysis, LVGLS RVGLS significantly associated ‐cause mortality. classified 4 strain groups according LVGLS (≥9%) RVGLS (≥12%). Cox proportional hazards analysis, group 4 (<9% LVGLS <12% RVGLS) worst prognosis, hazard ratio (HR) 1.755 (95% confidence interval [CI], 1.473–2.091; P<0.001) compared group 1 (≥9% LVGLS ≥12% RVGLS).Dividieron los pacientes en 2x2=4 grupos dependiendo de que los pacientes superasen o en dos variables ciertos puntos de corte elegidos por los investigadores y con algún significado clínico. Eligen un grupo de referencia que tiene el mejor pronóstico:“group 1 (≥9% LVGLS ≥12% RVGLS)”Con respecto él, miden cuánto más peligroso es estar en el resto de grupos.Es habitual hacerlo con conceptos que son razones, de forma que:una razón igual 1 refleja que algo es igual de peligroso;una razón igual 1 refleja que algo es igual de peligroso;valores mayores que 1 indican aumento del riesgo;valores mayores que 1 indican aumento del riesgo;valores menores que 1 indican disminución del riesgovalores menores que 1 indican disminución del riesgoEn este caso utilizan como medida de riesgo el Hazard Ratio (HR).En concreto, para el grupo que tenía peor pronóstico,group 4 (<9% LVGLS <12% RVGLS)un intervalo de confianza al 95% les indica que HR es un valor comprendido en 1.473–2.091. Es decir, saben seguro cual es el valor real, pero responden su calculo usando un mecanismo que acierta en el 95% de las ocasiones, y que en este caso indica que sea cual sea el valor real, este debe ser claramente mayor que 1.En términos de contrastes de hipótesis podríamos decirlo de este modo:Si hubiese datos que nos indicasen otra cosa, en principio asumiríamos la hipótesis nula de que los grupos 1 y 4 presentan mortalidad similar (HR=1)Si hubiese datos que nos indicasen otra cosa, en principio asumiríamos la hipótesis nula de que los grupos 1 y 4 presentan mortalidad similar (HR=1)Si los datos mostrasen otra cosa con evidencia suficiente podríamos decir que tenemos pruebas en contra de la nula, y favor de una Hipótesis alternativa: Los grupos 1 y 4 presentan mortalidad diferente (HR≠1).Si los datos mostrasen otra cosa con evidencia suficiente podríamos decir que tenemos pruebas en contra de la nula, y favor de una Hipótesis alternativa: Los grupos 1 y 4 presentan mortalidad diferente (HR≠1).Pues bien, la evidencia que ellos encuentran en contra de la hipótesis nula es:95% confidence interval [CI], 1.473–2.091; P<0.001Es decir, si fuese cierta la nula y ambos grupos fuesen en realidad de riesgo similar, ellos han obtenido una muestra tan extraña que es dificil de explicar menos que el azar haya jugado en su contra de tal modo que les ha ocurrido algo que se vería ni en 1 de cada 1000 estudios similares.Más adelante en el artículo indican:basis previously reported data, calculated sample\nsize obtain hazard ratio (HR) 1.3 groups. 2-sided log-rank test overall sample size 1600 participants (800 group 1 800 group 2) achieved\n99.1% power 0.05 significance level detect HR \n1.30 control group HR 1.Se podría traducir de este modo. Tenemos como hipótesis alternativa la de que ciertos grupos de pacientes deben tener un HR>1.3, que es un valor que el investigador considera clínicamente relevante. ha sido observado (aún) en los datos ya que cuando diseñaron el estudio, habían empezado recoger datos de los pacientes). Es un valor que el investigador, por su experiencia en el tema, sospecha que debe ser así de grande o aún mayor. Es, en la fase de diseño del estudio, solo una creencia del investigador, basada en lo que sabe o ha leído del tema.Eligieron una muestra de 1600 pacientes, que es lo bastante grande como para que si su hipótesis alternativa fuese cierta con HR>1.3, pudieran declarar significativo el resultado con una probabilidad del 99.1%. Esto era algo conocido antes de realizar el estudio. Como observa, la potencia se utiliza para diseñar el estudio. Nadie quiere empezar trabajar en un tema donde si tu hipótesis alternativa es correcta, por haber elegido la muestra muy pequeña, tienes pocas probabilidades de encontrar evidencia contra la hipótesis nula.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","text":"En este capítulo veremos procedimientos para contrastar si las diferencias numéricas obtenidas al comparar dos tratamientos (o dos poblaciones) son lo suficientemente grandes como para que su única causa sea atribuible al azar.Este tipo de pruebas se suelen usar cuando se elige una muestra de individuos que han seguido cierto tratamiento (por ejemplo, un placebo), y otra muestra que ha recibido otro tratamiento (por ejemplo, un fármaco de pruebas).Dependiendo de cómo se construyan ambas muestras, clasificamos el experimento en dos clases:Muestras apareadas: Cuando en realidad hay dos grupos de pacientes, si que solo hay un grupo de ellos, pero de cada individuo se tienen dos medidas. Esto puede ser debido por ejemplo que se ha medido cada uno en dos oacasiones (antes y después de un tratamiento) o se ha medido al pacientes con dos aparatos que deberían haber dado un resultado similar.Muestras apareadas: Cuando en realidad hay dos grupos de pacientes, si que solo hay un grupo de ellos, pero de cada individuo se tienen dos medidas. Esto puede ser debido por ejemplo que se ha medido cada uno en dos oacasiones (antes y después de un tratamiento) o se ha medido al pacientes con dos aparatos que deberían haber dado un resultado similar.Muestras independientes: Este es el caso más común, en el que realmente si hay dos grupos diferentes de individuos, cada uno con una medición numérica. Se supone que los individuos de un grupo de tratamiento han sido extraídos independientemente de los del otro.Muestras independientes: Este es el caso más común, en el que realmente si hay dos grupos diferentes de individuos, cada uno con una medición numérica. Se supone que los individuos de un grupo de tratamiento han sido extraídos independientemente de los del otro.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"muestras-apareadas-o-relacionadas","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"4.1 Muestras apareadas o relacionadas","text":"Aparecen por ejemplo en los estudios de pacientes “antes y después”, es decir, cuando en un una base de datos de de pacientes tenemos dos columnas de datos numéricos, una correspondientes los valores antes del tratamiento y otra correspondientes al valor después. En este caso también decimos que cada individuo es su propio control.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"hay-otros-ejemplos-de-estudios-con-apareamiento","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"Hay otros ejemplos de estudios con apareamiento","text":"Se deben realizar de modo apareados los estudios en que al medir una variable sospechamos de más fuentes de variabilidad además del tratamiento. Por ejemplo, si creemos que el sexo de los individuos puede influir notablemente en la variable que medimos y queremos anular el efecto tenemos varias posibilidades:Hacer un estudio con todos los individuos del mismo sexo en cada muestra, y otro con todos los individuos del otro sexo. esto se le denomina estratificar por sexos. En este caso lo que tenemos son dos estudios con muestras independientes.Hacer un estudio con todos los individuos del mismo sexo en cada muestra, y otro con todos los individuos del otro sexo. esto se le denomina estratificar por sexos. En este caso lo que tenemos son dos estudios con muestras independientes.Asociar cada individuo del primer grupo, un individuo “que se parezca” del segundo. Por ejemplo, si pensamos que conjuntamente el sexo y la edad pueden influir en el resultado, podemos hacer que por cada individuo del primer grupo, se elija un individuo del segundo, del mismo sexo y de edad similar. pesar de las dificultades que comporta hacer un estudio con esas características, habremos reducido la posibilidad de que las variables sexo y edad hayan influido mucho en las diferencias observadas entre tratamientos.Asociar cada individuo del primer grupo, un individuo “que se parezca” del segundo. Por ejemplo, si pensamos que conjuntamente el sexo y la edad pueden influir en el resultado, podemos hacer que por cada individuo del primer grupo, se elija un individuo del segundo, del mismo sexo y de edad similar. pesar de las dificultades que comporta hacer un estudio con esas características, habremos reducido la posibilidad de que las variables sexo y edad hayan influido mucho en las diferencias observadas entre tratamientos.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"hipótesis-nula-en-estudios-apareados","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"4.1.1 Hipótesis nula en estudios apareados","text":"En los contrastes con muestras apareadas, la hipótesis nula es que la distribución de los valores en una y otra medición son similares. Como en todo contraste de hipótesis, se declara que el efecto es estadísticamente significativo si la significación calculada es inferior cierta cantidad pequeña (5% o 1% típicamente).Los contrastes se realizan calculando las diferencias existentes entre cada observación de un grupo y la observación asociada en el segundo. Si las mencionadas diferencias tienen una distribución aproximadamente normal o bien la muestra es grande, la prueba que se suele usar es la t-student para muestras apareadas. Esta prueba funciona del siguiente modo. Si se dan las condiciones de validez (las diferencias son normales), las diferencias deberían ser aproximadamente normales de media cero. Si al calcular la media de las diferencias, el valor obtenido en la muestra es consistente con una posible media de cero, se rechaza la hipótesis nula. Es decir, si la diferencia entre lo observado y la hipótesis nula es atribuible al puro azar, aceptamos que hay diferencias entre los grupos.Una prueba menos exigente es el contraste paramétrico de Wilcoxon. Este último considera las diferencias entre cada observación y su control. Si la hipótesis nula fuese cierta, las diferencias negativas serían similares en cantidad y tamaño las diferencias positivas. La prueba de Wilcoxon examina la discrepancia existente entre los resultados observados y la predicción de la hipótesis nula.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"ejemplo","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"Ejemplo","text":"Se estudia si mejoran unos niños su velocidad de lectura en palabras por minuto. Para ellos se les mide al inicio de un estudio, se les vuelve medir tras una intervención. Se toma nota de la diferencia entre la velocidad final y la inicial. Puede descargar los datos del experimento en la base de datos lectura-anova.sav. La exploración de los primeros casos, ordenados de menor mayor diferencia nos ofrece:Ya que los datos están ordenados de forma que los peores resultados se muestran primero, y estos han experimentado mejoría, parece que evidentemente vamos encontrar gran diferencia en contra de la hipótesis nula. Veamos qué distribución presentan las diferencias:Se ve que todos los individuos han mejorado. la evidencia contra la hipótesis nula la encontramos en esta tablaLa diferencia observada en los niños representa una mejoría de más de 12 palabras por minuto. Sea cual sea la mejoría media si se aplicase en toda la población de niños similares estos, los datos observados están en contra de la hipótesis nula de mejoría (p<0.001). Tenemos una confianza del 95% de que sea cual sea la mejoría media, el valor real está en algún lugar entre 11 y 13 palabras por minuto (redondeando).En SPSS podemos realizar este calculo seleccionando el menú: “_Analizar - Comparar medias - prueba T para muestras apareadas”. Allí marcamos el par de variables Antes y Después. También podríamos haber elegido el menú”_Analizar - Comparar medias - prueba T para una muestra” y haber trabajado con la variable Dferencia. El resultado es el mismo.","code":"\ndf=read_sav(\"datos/lectura-anova.sav\", user_na=FALSE) %>% haven::as_factor() \ndf %>% head()  %>% knitr::kable(booktabs=T)\nggplot(df, aes(x=Diferencia))+geom_histogram(fill=\"lightblue\")+ geom_rug(sides = \"b\", aes(y = 0), position = \"jitter\", colour = \"blue\")+coord_cartesian(xlim=c(0,20))+geom_vline(xintercept = 0,lty=2,col=\"red\")\ndf %>% desc1vn(vNum = \"Diferencia\",columnas = c(\"mediaet\",\"p.intra\",\"ic1\",\"ic2\")) %>%\n  mutate(Variable=\"Diferencia\") %>% select(Variable,everything()) %>%\n  knitr::kable(booktabs=T,\n                col.names=c(\"Variable\",\"media±et\", \"p (signif.)\", \"ic95%(min)\", \"ic95%(max)\"))"},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"dos-muestras-independientes","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"4.2 Dos Muestras independientes","text":"Sirven para tratar problemas como el de si el nivel de hierro es similar en los individuos que padecen determinada enfermedad frente los individuos sanos. Para ello podríamos elegir una muestra de individuos enfermos y otra de sanos, y comparar si los valores de una muestra tienen tendencia ser mayores que los de la otra.En los contrastes con muestras independientes, la hipótesis nula es que los valores obtenidos en una y otra muestra son similares, frente la hipótesis alternativa de que son diferentes. El valor obtenido en la significación nos permite decidir si se rechaza o la hipótesis nula.Hay varias maneras de realizar este tipo de contrastes:Prueba t-student para dos medias: Se basa en contrastar si las medias de cada grupo son similares. Este tipo de contrastes es válido cuando se da alguna de las siguientes condiciones:Prueba t-student para dos medias: Se basa en contrastar si las medias de cada grupo son similares. Este tipo de contrastes es válido cuando se da alguna de las siguientes condiciones:Las desviaciones típicas/varianzas son similares y las observaciones de cada muestra son normales.Las desviaciones típicas/varianzas son similares y las observaciones de cada muestra son normales.Las desviaciones típicas/varianzas son similares y los tamaños muestrales son grandes.Las desviaciones típicas/varianzas son similares y los tamaños muestrales son grandes.Hay diferencia notable entre las varianzas de cada grupo, pero los tamaños de cada muestra son similares y además las muestras son grandes o aproximadamente normales.Hay diferencia notable entre las varianzas de cada grupo, pero los tamaños de cada muestra son similares y además las muestras son grandes o aproximadamente normales.Pruebas paramétricas para muestras independientes: requieren ningún tipo de suposición sobre la distribución de las muestras. Esto permite que se puedan usar con variables discretas u ordinales. Pruebas populares son las de Mann-Whitney, Wilcoxon o la prueba de Kolmogorov-Smirnov de dos muestras.Pruebas paramétricas para muestras independientes: requieren ningún tipo de suposición sobre la distribución de las muestras. Esto permite que se puedan usar con variables discretas u ordinales. Pruebas populares son las de Mann-Whitney, Wilcoxon o la prueba de Kolmogorov-Smirnov de dos muestras.En el caso de que queramos aplicar la prueba t-student, debemos tener en cuenta que esta se ve muy afectada en caso de que las muestras estén similarmente dispersas, y es necesario hacer una corrección. Debemos comprobar que existan observaciones anómalas. Disponemos del contraste de Levene para la igualdad de varianzas. De hecho, SPSS lo realiza automáticamente siempre que se desea realizar la prueba t-student, y muestra los resultados para la prueba t-student, tanto en el caso de que el usuario desee admitir la igualdad de varianzas, como rechazarla. Se suele recomendar al usuario elegir un nivel de significación alto para esta prueba (hasta el 15%).","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"ejemplo-1","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"Ejemplo","text":"Se cree que la ingesta de calcio reduce la presión sanguínea. Para contrastarlo se decidió elegir 21 individuos de características similares para participar en un estudio. 10 de ellos elegidos al azar, se les asignó un tratamiento consistente en un suplemento de calcio durante 3 meses y se observó la diferencia producida en la presión arterial (la que había “antes” menos la que había “después”). los 11 individuos restantes se les suministró un placebo y se midió también la diferencia. Los datos podemos encontrarlos en la base de datos [calcio.sav]{datos/calcio.sav}, de la que exploramos las primeras líneas:solo nos interesa la variable Diferencia comprarada entre ambos grupos, pero de todas formas vamos examinar las mediciones Antes. La razón es que al realizar un experimento, repartimos los individuos en dos grupos, y estamos muy interesados en que los dos grupos sean muy similares en el punto de partida (medición basal):\nSe aprecia que los dos grupos eran similares al iniciarse el experimento. Sin embargo el grupo de que tomo el Placebo practicamente se quedó igual, y el que tomó Calcio experimentó una mejoría. Nos queda por saber si esta mejoría es explicable por el azar o va más allá (diferencia estadísticamente significativa entre grupos).la vista de los resultados, entre el grupo Calcio y el Placebo hay diferencias estadísticamente significativas, solo al inicio del experimento, si que el cambio producido en los individuos también es compatible con la hipótesis nula de que obtienen resultados similares. Las significaciones obtenidas se han realizado tanto con la prueba paramétrica de la t-student como con una prueba paramétrica. Ambas obtienen conclusiones similares.Podemos realizar la prueba paramétrica en SPSS en el menú: “Analizar - Comparar medias -Prueba t para muestras independientes”. En la variable Grupo hemos de indicar que comparamos los grupos 0 (Placebo), con el grupo 1 (Calcio), pues así es como se han codificado numéricamente las variables. El el campo de Variables de prueba situamos las variables Antes y Diferencia.La prueba paramétrica está en un diálogo análogo que obtenemos en el menú “_Analizar - Pruebas paramétricas - Cuadros de dialogo antiguos - 2 muestras independientes”Una representación gráfica de los intervalos de confianza muestra que ambos tienen el común el valor cero. Es decir, ninguno de ellos puede decirse que presente una mejoría significativa. ¿Podría interpretarse como que entre ellos hay diferencia? Esto es algo que se deduce directamente mirando los intervalos de confianza, aunque nuestra intuición nos indique que deben haber mucha diferencia entre ellas.","code":"\ndf=read_sav(\"datos/calcio.sav\", user_na=FALSE) %>% haven::as_factor()\ndf %>% head()  %>% knitr::kable(booktabs=T)\ngrid.arrange(\nggplot(df,aes(x=Grupo,y=Antes))+geom_boxplot(fill=\"lightblue\"),\nggplot(df,aes(x=Grupo,y=Diferencia))+geom_boxplot(fill=\"lightblue\"),\nncol=2)\ndf %>% generaTablatTestPorGrupo(\"Grupo\", c(\"Antes\",\"Diferencia\"),\n                                columnas = c(\"n\",\"mediaet\",\"p.t\",\"ci95\", \"p.w\")) %>% \n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                        \"n\",\"media±et\", \n                        \"n\",\"media±et\",\n                        \"p (parám.)\",\"ic95% dif.\", \"P (no parám).\")) %>%\n  add_header_above(c(\" \" = 1, \"Placebo\" = 2, \"Calcio\" = 2, \" \"=3))\nresumen=df %>% gather(Variable,Valor,-Grupo) %>% \n  group_by(Grupo,Variable) %>% \n  summarise(Media=mean(Valor),\n                n=length(Valor),\n               IC=sd(Valor)/sqrt(n)*qt(0.975,n-1))\nggplot(resumen %>% filter(Variable==\"Diferencia\"), aes(x=Grupo,y=Media)) +\n  geom_errorbar(aes(ymin=Media-IC,ymax=Media+IC),width=0.2, size=1, color=\"navyblue\")+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(\"Diferencia\")+\n  geom_hline(yintercept=0,lty=2,color=\"red\")"},{"path":"diferencias-que-presenta-una-variable-numérica-entre-dos-grupos.html","id":"es-lo-mismo-que-la-diferencia-entre-dos-grupos-sea-significativa-que-el-que-sus-respectivos-intervalos-de-confianza-para-la-media-no-se-toquen","chapter":"Capítulo 4 Diferencias que presenta una variable numérica entre dos grupos","heading":"¿Es lo mismo que la diferencia entre dos grupos sea significativa que el que sus respectivos intervalos de confianza para la media no se toquen?","text":"Parecería que es lo mismo, pero es así. La realidad es que si hay diferencia significativa entre la media de dos grupos, entonces los intervalos de confianza se tocan nada o muy poco. Vamos ilustrarlo retomando la base de datos 2poblaciones-Mismotrabajo-DiferenteNutricion.sav.Vamos usar tanto las pruebas t-student como las paramétricas para estudiar las diferencias entre los individuos de Málaga y Tanger en todas las variables numéricas.Normalmente en una publicación científica pondríamos como tabla de resultados algo similar esto:Normalmente mostraríamos los dos tipos de significación si que haríamos la elección de usar pruebas paramétricas o paramétricas según se den las condiciones de validez. Obsérvese que cuando las muestras son pequeñas y las desviaciones de la normalidad son grandes, ambos tipos de prueba suelen ofrecer significaciones similares.Veamos para varias de estas variables como son los ic95% para las medias en cada grupo, y para las diferencias entre ambos grupos. La diferencia entre ambos grupos será significativa cuando el IC)%% para la diferencia contiene al cero. En ese caso observaremos que los IC95% para cada grupo se tocan nada o muy poco.Para el colesterol se ve que el IC95% para la diferencia contiene al valor cero, es decir, tenemos evidencia estadísticamente significativa en contra de que las medias en ambos grupos sean iguales. Por otro lado, vemos que los IC95% para las medias de colesterol en ambos grupos se cruzan. Hasta ahora todo coincide con la intuición.En cuanto la siguiente variable, Triglicéridos y Glucemia nos encontramos la sorpresa: observamos que el IC95% para la diferencia de medias contiene (por poco) al cero, es decir, la diferencia de medias es apenas significativamente diferente de cero, sin embargo los intervalos de confianza dentro de cada grupo se cruzan (por poco).Estudie lo que ocurre en el resto de variables y compruebe si gráficamente es cierto lo de que:Cuando la diferencia entre dos grupos es estadísticamente significativa, los IC95% de cada grupo se tocan nada o muy poc.oEn las publicaciones suele verse un tipo de gráfico u otro: El que muestra los intervalos de confianza en cada grupo, o el que sirve para estudiar las diferencias entre dos grupos. Existe una tercera posibilidad, usando la libreria emmeans de R, donde además de observarse los intervalos de confianza para la media en cada grupo, una flecha interior indica cuando las comparaciones entre dos grupos son significativas: Ocurre justo cuando tienen intersección común. Este tipo de gráficos será muy útil cuando haya muchas comparaciones realizar entre grupos (análisis posthoc de ANOVA).","code":"\nvNumericas=names(df) %>% setdiff(\"Grupo\")\ndf %>% generaTablatTestPorGrupo(\"Grupo\", vNumericas,\n                                columnas = c(\"n\",\"mediaet\",\"p.t\",\"ci95\",\"p.w\")) %>% \n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                        \"n\",\"media±et\", \n                        \"n\",\"media±et\",\n                        \"p dif.\",\"IC95% dif.\", \"p\")) %>%\n  add_header_above(c(\" \" = 1, \"Tanger\" = 2, \"Málaga\" = 2,\"t-test\"=2, \"No normal\"=1)) %>%\n  kable_styling(font_size=12)\n\ndfResumen=df %>% generaTablatTestPorGrupo(\"Grupo\", vNumericas,\n                                columnas = c(\"ic1\",\"ic2\", \"ci_min\",\"ci_max\")) \n\ndfResumenTidy = dfResumen %>% select(-ci_min,-ci_max) %>% gather(Clave,Valor,-Variable) %>%\n  separate(Clave,c(\"Concepto\",\"Grupo\")) %>% spread(Concepto,Valor)\n\n\ndfResumen %>% \n  knitr::kable(booktabs=T, \n                col.names=c(\"Variable\",\n                        \"ic(min)\",\"ic(max)\" ,\n                        \"ic(min)\",\"ic(max)\",  \n                         \"ic.dif.(min)\", \"ic.dif.(max)\")) %>%\n  add_header_above(c(\" \" = 1, \"Tanger\" = 2, \"Málaga\" = 2,\"Málaga-Tanger\"=2))\nlaVariable=\"Colesterol\"\ngrid.arrange(\nggplot(dfResumenTidy %>% filter (Variable==laVariable), aes(x=Grupo, y=(ic1+ic2)/2)) +\n  geom_errorbar(aes(ymin=ic1,ymax=ic2),width=0.2, size=1, color=\"navyblue\")+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(laVariable)+xlab(\"\"),\nggplot(dfResumen %>% filter (Variable==laVariable), aes(x=\"Málaga-Tanger\", y=(ci_max+ci_min)/2)) +\n  geom_errorbar(aes(ymin=ci_min,ymax=ci_max),width=0.2, size=1, color=\"navyblue\")+\n  geom_point( size=4, shape=21, fill=\"white\")+ylab(str_c(\"Diferencia de \", laVariable)) +\n  xlab(\"\")+geom_hline(yintercept=0,lty=2,color=\"red\"),\nncol=2)\nlistaGraficos=vNumericas %>% map( ~ lm(formula(str_c(.,\" ~ Grupo\")),data=df) %>%\n              emmeans(\"Grupo\") %>% plot(comparisons=TRUE)+xlab(.x)+ylab(\"\")+coord_flip())\ndo.call(\"grid.arrange\", c(listaGraficos, ncol=3))"},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","text":"En este capítulo vamos generalizar las ideas del anterior, donde comparábamos las diferencias observadas en una variable en dos grupos, al caso de más grupos. De hecho, si aplicamos algunas de las técnicas que aquí veremos sólo dos grupos, darían los mismos resultados que técnicas tratadas con anterioridad.Como ilustración del tipo de problemas que podemos tratar, está el de comparar los resultados de 4 tratamientos. Uno podría ser un placebo aplicado un grupo de control, y los otros tres podrían corresponder sendas dosis de un medicamento. La variable respuesta sería una variable numérica.Al igual que en el capítulo anterior consideraremos dos tipos de técnicas:Paramétricas: Cuando suponemos que las distribución de los datos de cada muestra tienen cierta distribución particular. La técnica que mostraremos se denomina ANOVA de una vía.Paramétricas: Cuando suponemos que las distribución de los datos de cada muestra tienen cierta distribución particular. La técnica que mostraremos se denomina ANOVA de una vía.paramétricas: es necesario suponer nada sobre la distribución de los datos. Interpretaremos la prueba de Kruskal-Wallis, que puede extenderse variables ordinales.paramétricas: es necesario suponer nada sobre la distribución de los datos. Interpretaremos la prueba de Kruskal-Wallis, que puede extenderse variables ordinales.Sea cual sea el contraste que utilicemos, la hipótesis nula traducirá la idea de que en los diferentes grupos se obtienen resultados similares (“efecto”), y la hipótesis alternativa lo negará. La significación del contraste nos dará una idea de si las diferencias observadas en los diferentes grupos son imputables al azar (significación grande) o hay una diferencia intrínseca entre algunos grupos (significación pequeña).","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"anova-de-un-factor-o-una-vía","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1 Anova de un factor o una vía","text":"Para usar el modelo ANOVA debe poder suponerse que son válidas una serie de condiciones:La variabilidad de todas las muestras debe ser similar. Ésta es la condición más importante.La variabilidad de todas las muestras debe ser similar. Ésta es la condición más importante.Las muestras deben tener una distribución aproximadamente normal. Cierto alejamiento de esta hipótesis es muy problemático.Las muestras deben tener una distribución aproximadamente normal. Cierto alejamiento de esta hipótesis es muy problemático.Los tamaños de las muestras deben ser muy dispares. Esta condición en realidad es estrictamente necesaria, y además es controlable al realizar un experimento. Pero, si se sospecha una cierta heterogeneidad en la variabilidad en los diferentes grupos (como es razonable esperar en la práctica), el tener grupos desequilibrados en tamaño sólo puede empeorar las cosas. Esta condición simplifica el poder interpretar los resultados.Los tamaños de las muestras deben ser muy dispares. Esta condición en realidad es estrictamente necesaria, y además es controlable al realizar un experimento. Pero, si se sospecha una cierta heterogeneidad en la variabilidad en los diferentes grupos (como es razonable esperar en la práctica), el tener grupos desequilibrados en tamaño sólo puede empeorar las cosas. Esta condición simplifica el poder interpretar los resultados.Si se dan las dos primeras condiciones, el modelo ANOVA contrasta la hipótesis nula de que en todos los grupos se obtienen valores similares de las variables por una condición equivalente: Que las medias (como parámetros) en los diferentes grupos son iguales.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"en-qué-se-basa-el-contraste-anova","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1.1 En qué se basa el contraste ANOVA","text":"Si suponemos que la hipótesis nula de la igualdad de medias en los diferentes grupos es cierta, podríamos decir que todas las observaciones pueden considerarse que provienen de un único grupo cuya media y variabilidad es la misma que la de cualquiera de los grupos por separado. Por tanto observamos que hay diferentes maneras de estimar la variabilidad en la población. De las discrepancias entre diferentes estimaciones de la variabilidad surge toda una familia de técnicas conocidas como análisis de la varianza, de las cuales el ANOVA de una vía es más que el representante más simple.La cuestión es que si alguno de los grupos presenta unos valores que en media se alejan del resto, esto se apreciará en el contraste como una fuente extra de variabilidad explicable por el azar. La significación del contraste se calcula evaluando si esta variabilidad extra es muy grande con respecto una variabilidad que sería de esperar si la hipótesis nula fuese cierta. Es por ello, que al realizar un contraste ANOVA siempre veremos varias fuentes de variabilidad. Los detalles son engorrosos, y en esta explicación resumida de la técnica nos limitaremos tomar la decisión de rechazar o aceptar la hipótesis nula en función de la significación del contraste (que es fácilmente identificable).Suele llevar confusión el nombre, pues hace pensar que estamos contrastando igualdad de varianzas, pero en realidad esto es algo que se supone. Lo que contrastamos es la igualdad de medias.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"cómo-se-interpreta-anova","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1.2 Cómo se interpreta ANOVA","text":"Si al realizar la prueba ANOVA se obtiene una significación baja (por ejemplo p< 0.05) rechazaremos la hipótesis de que en todos los grupos las medias son iguales. La siguiente cuestión que aparece de modo natural en esta situación es la de identificar en qué grupos se han producido las diferencias. Básicamente tenemos las siguientes aproximaciones para abordar la cuestión:Cuando tenemos una idea previa de en qué grupos eran de esperar las mayores diferencias, utilizamos los contrastes planeados o contrastes post-hoc.Cuando tenemos una idea previa de en qué grupos eran de esperar las mayores diferencias, utilizamos los contrastes planeados o contrastes post-hoc.Antes de recoger datos es posible que tengamos algunas sospechas de dónde se deberían producir las diferencias. esto se le denomina comparaciones planeadas. En SPSS lo encontramos pulsando el botón “contrastes” en la ventana para realizar el contraste ANOVA.Antes de recoger datos es posible que tengamos algunas sospechas de dónde se deberían producir las diferencias. esto se le denomina comparaciones planeadas. En SPSS lo encontramos pulsando el botón “contrastes” en la ventana para realizar el contraste ANOVA.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"contrastes-no-planeados-o-post-hoc","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1.2.1 Contrastes no planeados o post-hoc","text":"Bajo ese nombre encontramos múltiples técnicas. Éstas se consideran bastante conservadoras, en el sentido de que intentan reducir la posibilidad de errores de tipo , costa de aumentar la posibilidad de errores de tipo II. Dicho de otro modo, es probable que en situaciones donde realmente haya diferencias entre grupos, las pruebas post-hoc lo detecten. Tienen que ser las diferencias entre grupos realmente grandes para poder ser reconocidas por estas pruebas.Como hemos mencionado, dentro de la categoría de contrastes planeados, hay muchas técnicas disponibles. Debe ser por algo. continuación presentamos unas indicaciones que nos pueden ayudar decidir cuál se adecua mejor un estudio si estamos usando SPSS. Sólo indicaremos las más populares.Una primera división entre estas pruebas es:Tests de rangos: Son aquellas que buscan identificar grupos homogéneos (medias parecidas).Tests de rangos: Son aquellas que buscan identificar grupos homogéneos (medias parecidas).Comparaciones múltiples: Buscan establecer diferencias entre grupos basándose en diferencias dos dos.Comparaciones múltiples: Buscan establecer diferencias entre grupos basándose en diferencias dos dos.Esta división es estricta pues hay técnicas que se pueden incluir en esas dos categorías la vez.Grupos equilibrados y varianzas similares: Cuando todos los grupos tienen el mismo número de individuos, y podemos asumir que están igualmente dispersos.\no Diferencia Honestamente Significativa de Tukey (HSD de Tukey). Se puede considerar la vez como una técnica de comparaciones múltiples y la vez de rangos. Es un test que se suele utilizar cuando se quiere comparar cada grupo con todos los demás y el número de grupos es alto (6 o más). Es una prueba conservadora (mantiene bajo el error de tipo , sacrificando la capacidad de detectar diferencias existentes).\no Test de Scheffé: Hace todas las comparaciones posibles. Por ejemplo, el primer grupo con respecto cada uno de los restantes, pero también el primero con respecto al grupo formado por la unión de dos de los restantes, etc. Si sólo se desea comparar grupos dos dos es preferible la prueba HSD de Tukey, pero si las comparaciones que buscamos son más complejas, preferimos esta prueba.Grupos equilibrados y varianzas similares: Cuando todos los grupos tienen el mismo número de individuos, y podemos asumir que están igualmente dispersos.\no Diferencia Honestamente Significativa de Tukey (HSD de Tukey). Se puede considerar la vez como una técnica de comparaciones múltiples y la vez de rangos. Es un test que se suele utilizar cuando se quiere comparar cada grupo con todos los demás y el número de grupos es alto (6 o más). Es una prueba conservadora (mantiene bajo el error de tipo , sacrificando la capacidad de detectar diferencias existentes).\no Test de Scheffé: Hace todas las comparaciones posibles. Por ejemplo, el primer grupo con respecto cada uno de los restantes, pero también el primero con respecto al grupo formado por la unión de dos de los restantes, etc. Si sólo se desea comparar grupos dos dos es preferible la prueba HSD de Tukey, pero si las comparaciones que buscamos son más complejas, preferimos esta prueba.Grupos desequilibrados: Cuando tenemos un número diferente de individuos en cada grupo puede interesarnos elegir alguna de las siguientes pruebas:Grupos desequilibrados: Cuando tenemos un número diferente de individuos en cada grupo puede interesarnos elegir alguna de las siguientes pruebas:LSD de Fisher (sólo si hay 3 grupos),LSD de Fisher (sólo si hay 3 grupos),T3 de Dunnett,T3 de Dunnett,C de Dunnett,C de Dunnett,Scheffé,Scheffé,Games-Howell.Games-Howell.Varianzas desiguales: Cuando la prueba de igualdad de varianzas (por ejemplo Levene) nos hace sospechar que las varianzas son similares en todos los grupos, podemos considerar alguna de estas pruebas:Varianzas desiguales: Cuando la prueba de igualdad de varianzas (por ejemplo Levene) nos hace sospechar que las varianzas son similares en todos los grupos, podemos considerar alguna de estas pruebas:T2 de TamhaneT2 de TamhaneT3 de Dunnett,T3 de Dunnett,C de Dunnett,C de Dunnett,Scheffé,Scheffé,Games-Howell.Games-Howell.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"comparaciones-planeadas","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1.2.2 Comparaciones planeadas","text":"Son las que deberíamos hacer cuando honestamente tenemos una sospecha sobre el posible resultado del análisis, es decir entre qué grupos esperamos encontrar diferencias, y estas sospechas han sido formuladas con anterioridad la recogida de datos.Esto ocurre por ejemplo cuando un grupo de control se le aplica un placebo, al segundo cierta cantidad de un medicamento experimental, y al tercero una cantidad mayor, pero que esperamos en principio que mejore en mucho los resultados del segundo grupo. Posiblemente sospechemos que el grupo control va obtener resultados muy diferentes al segundo y al tercero. pensamos encontrar diferencias entre el segundo y el tercero. Este es el tipo de cuestiones que podemos resolver con las comparaciones planeadas.\nEstas pruebas se realizan de la siguiente manera usando SPSS:En la ventana donde se realiza el contraste ANOVA (“Analizar – Comparar medias – ANOVA de un factor…”) pulsamos en el botón etiquetado “contrastes”. Allí marcamos la casilla “polinómicos”.En la ventana donde se realiza el contraste ANOVA (“Analizar – Comparar medias – ANOVA de un factor…”) pulsamos en el botón etiquetado “contrastes”. Allí marcamos la casilla “polinómicos”.Tomamos nota de un grupo (o más si pensamos que pueden ofrecer resultados parecidos) para compararlos con otro grupo (o más) que pensamos que ofrecerán resultados diferentes.Tomamos nota de un grupo (o más si pensamos que pueden ofrecer resultados parecidos) para compararlos con otro grupo (o más) que pensamos que ofrecerán resultados diferentes.cada uno de los primeros les asociamos unos coeficientes positivos que reflejen la importancia de ese grupo, y los grupos con los que queremos contrastarlos les asociamos unos coeficientes negativos. Esto se hace de modo que al sumarlos todos los coeficientes obtengamos un total de cero.cada uno de los primeros les asociamos unos coeficientes positivos que reflejen la importancia de ese grupo, y los grupos con los que queremos contrastarlos les asociamos unos coeficientes negativos. Esto se hace de modo que al sumarlos todos los coeficientes obtengamos un total de cero.Pulsando en “siguiente” podemos ir añadiendo todas las hipótesis que queramos.Pulsando en “siguiente” podemos ir añadiendo todas las hipótesis que queramos.Por ejemplo si pensamos que el primer grupo es diferente del segundo y el tercero, podemos asociarle al primer grupo un coeficiente 1 y -0.5 cada uno de los otros dos; Así indicamos que queremos contrastar el primer grupo frente al conjunto formado por los dos restantes.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"qué-hacer-si-no-se-verifican-las-premisas-del-modelo-anova","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.1.3 ¿Qué hacer si no se verifican las premisas del modelo ANOVA?","text":"Hemos mencionado que para que el contraste ANOVA sea válido es necesario que podamos suponer una serie de propiedades sobre la distribución de la variable en los diferentes grupos.La más importante es la de igualdad de las varianzas. Para ello podemos prestar atención al resultado de la prueba de Levene. Dado lo sensible que es el contraste ANOVA la falta de este requisito, se acostumbra rechazar la igualdad de varianzas con un nivel de significación superior al habitual (el 0.15=15% por ejemplo). En caso de que se pueda admitir la igualdad de varianzas podemos usar las pruebas de Welch o de Brown-Forsythe para decidir sobre la igualdad de medias. En SPSS estas últimas, junto la prueba de Levene, las encontramos al pulsar el botón “opciones…” que aparece en la misma ventana en la que se hace la prueba ANOVA.Otra condición que necesitamos para la validez de la prueba ANOVA es la de la normalidad en cada uno de los grupos, aunque indicábamos que esta condición es tán crítica como la de la igualdad de varianzas. Si la variable que medimos es un parámetro biológico, con frecuencia tiene sentido hacer un cambio de variable logarítmico, que habitualmente normaliza las observaciones e iguala varianzas.","code":""},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"ejemplo-paramétrico","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"Ejemplo paramétrico","text":"Se realizó un experimento para comparar tres métodos de aprendizaje de lectura. Se asignó aleatoriamente los estudiantes cada uno de los tres métodos. Cada método fue probado con 22 estudiantes (experimento equilibrado). Se evaluó mediante diferentes pruebas la capacidad de comprensión de los estudiantes, antes y después de recibir la instrucción. Los resultados se recogen en la base de datos lectura-anova.sav, de la que exploramos las primeras líneas:Queremos contrastar la hipótesis nula de que las tres técnicas ofrecen resultados medios similares, frente la hipótesis alternativa de que esto es así. Para ello podríamos considerar el resultado obtenido “después”. Pero esto estaría sin duda influido por la habilidad previa de cada estudiante. Para eliminar esta fuente de variabilidad podríamos considerar cada estudiante como su propio control, y considerar una nueva variable que sería la diferencia entre “después” y “antes”. Ahora la hipótesis nula es que la variable “diferencia”, posee medias similares, frente la hipótesis alternativa de que esto es falso. Un análisis descriptivo exploratorio muestra que aparentemente el grupo control tiene resultados peores que los que siguieron la técnica y II.La siguiente tabla muestra los resultados de la prueba ANOVA para las 3 variables, en morma de media y error estándar, así como las comparaciones paramétricas (ANOVA) y paramétricas (Kruskal-Wallis, que veremos posteriormente):Las puntuaciones antes de recibir la enseñanza eran de media muy similares. La prueba ANOVA con dichas puntuaciones mostró diferencia estadísticamente significativa (p=0.45) como sería de esperar en una asignación aleatoria al grupo. Al observar la variable Después, La prueba ANOVA sí resulta significativa, es decir, los 3 grupos presentan el mismo cambio medio.El análisis posthoc lo podemos hacer con la prueba HSD (Diferencia Honestamente Significativa) de Tukey,Donde vemos para cada par de variables, la diferencia entre las medias, un intervalo de confianza (con extremos inferior y superior para la diferencia de medias), y la significación de la diferencia. En él vemos que el grupo de Control presenta diferencias significativas con respecto los demás grupos, que presentan diferencias significativas entre sí.Observe que en todos los intervalos de confianza donde se ha obtenido diferencias estadísticamente significativas para parejas de grupos, se obtienen intervalos de confianza que contienen el valor cero para la diferencia de medias. Esto es otra manera de leer un contraste de hipótesis que permite obtener una interpretación clínica de los resultados de un experimento más interesante que la más abstracta significación.El análisis post-hoc viene reflejado en el siguiente gráfico en el que se usa la prueba HSD para contrastar las diferencias entre pares de grupos, usando una flecha roja dentro del intervalo de confianza en cada grupo. Si las líneas rojas se superponen, la diferencia es significativa.En cuanto las condiciones de validez de la prueba ANOVA, veamos como podemos explorarlo:Normalidad: Podemos estudiar la normalidad en cada una de los grupos:Al estudiar muchos gráficos la vez, es posible que en algunos encontremos desviaciones de la normalidad que nos hagan dudar. Es normal por la abundancia de comparaciones múltiples. Es más conveniente es tratar todos los datos juntos estudiando todos los residuos del ajuste ANOVA (desviación de cada dato con respecto la media de su propio grupo), en un solo gráfico QQ:se aprecia una notable desviación de la normalidad.En cuanto la homogeneidad de varianzas (heterocedasticidad), podemos usar la prueba de Levene:Al haber obtenido p bastante grande para la prueba de homogeneidad de varianzas, podemos asumir que las tres muestras presentan similar dispersión.Como se aprecia es algo pesado ir comprobando todas las cuestiones de validez una una. La librería de R gvlma hace una buena cantidad de comprobaciones de una forma muy simple:Para hacer algo similar en SPSS tenemos el menú: * Analizar -Comparar medias - ANOVA de un factor. Las variables dependientes son Antes* y Diferencia. El factor es Grupo. En pruebas posthoc, podemos seleccionar Tukey. En el botón Opciones, podemos marcar la prueba de homogeneidad de varianzas. La normalidad en cada muestra puede ser comprobada como hicimos en ocasiones anteriores en el menú, Analizar - Estadísticos descriptivos - Explorar, eligiendo como gráfico el histograma y añadiéndole las pruebas de normalidad.","code":"\ndf=read_sav(\"datos/lectura-anova.sav\", user_na=FALSE) %>% haven::as_factor() \ndf %>% head()  %>% knitr::kable(booktabs=T)\ngeneraTablaANOVA1F(df,\"Grupo\",c(\"Antes\",\"Despues\",\"Diferencia\"),columnas = c(\"mediaet\",\"p.F\",\"p.kw\"))  %>%\n  knitr::kable( booktabs = T, \n                col.names=c(\"Variable\",\n                         \"Control\", \n                        \"Técnica I\",\n                        \"Técnica II\",\n                        \"p (parám.)\", \"P (no parám).\"))\n#> Error in omega_sq(modelo, ci.lvl = 0.95) : \n#>   could not find function \"omega_sq\"\n#> Error in omega_sq(modelo, ci.lvl = 0.95) : \n#>   could not find function \"omega_sq\"\n#> Error in omega_sq(modelo, ci.lvl = 0.95) : \n#>   could not find function \"omega_sq\"\nmodeloLineal <- lm(Diferencia ~ Grupo,data=df)\nmodeloLineal %>% aov() %>% TukeyHSD() %>% .[[\"Grupo\"]] %>% knitr::kable()\nmodeloLineal %>% emmeans(\"Grupo\") %>% \n                plot(comparisons=TRUE)+xlab(\"Diferencia\")+ylab(\"\")+coord_flip()\ngrid.arrange(\nggplot(df %>% filter(Grupo==\"Control\"),aes(sample=Diferencia))+stat_qq() + stat_qq_line()+ggtitle(\"Controles\"),\nggplot(df %>% filter(Grupo==\"Técnica I\"),aes(sample=Diferencia))+stat_qq() + stat_qq_line()+ggtitle(\"Técnica I\"),\nggplot(df %>% filter(Grupo==\"Técnica II\"),aes(sample=Diferencia))+stat_qq() + stat_qq_line()+ggtitle(\"Técnica II\"),\nncol=2)\nqqPlot(modeloLineal, main=\"QQ Plot\") #> [1] 12 65\nleveneTest(modeloLineal)\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)\n#> group  2    1.27   0.29\n#>       63\n(modeloLineal %>% gvlma() %>% summary()) %>% knitr::kable(booktabs=T)\n#> \n#> Call:\n#> lm(formula = Diferencia ~ Grupo, data = df)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.371 -1.996  0.121  2.098  6.129 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        9.871      0.579   17.04  < 2e-16 ***\n#> GrupoTécnica I     3.629      0.819    4.43  3.8e-05 ***\n#> GrupoTécnica II    3.220      0.819    3.93  0.00021 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.7 on 63 degrees of freedom\n#> Multiple R-squared:  0.272,  Adjusted R-squared:  0.249 \n#> F-statistic: 11.8 on 2 and 63 DF,  p-value: 4.54e-05\n#> \n#> \n#> ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n#> USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n#> Level of Significance =  0.05 \n#> \n#> Call:\n#>  gvlma(x = .) \n#> \n#>                       Value p-value                Decision\n#> Global Stat        1.89e+00   0.756 Assumptions acceptable.\n#> Skewness           1.11e-02   0.916 Assumptions acceptable.\n#> Kurtosis           1.87e+00   0.171 Assumptions acceptable.\n#> Link Function      3.13e-15   1.000 Assumptions acceptable.\n#> Heteroscedasticity 6.81e-03   0.934 Assumptions acceptable."},{"path":"diferencias-que-presenta-una-variable-numérica-entre-varios-grupos.html","id":"contraste-no-paramétrico-de-kruskal-wallis","chapter":"Capítulo 5 Diferencias que presenta una variable numérica entre varios grupos","heading":"5.2 Contraste no paramétrico de Kruskal-Wallis","text":"Si tenemos dudas sobre la validez de las condiciones del ANOVA, o incluso si la variable respuesta es ordinal, la prueba paramétrica de Kruskal-Wallis contrasta unas hipótesis análogas la prueba ANOVA de un factor. requiere condiciones de validez especiales.El contraste de Kruskal-Wallis contrasta que las medias sean iguales, sino simplemente si los valores obtenidos en los diferentes grupos son similares. Esto lo realiza de un modo sorprendentemente simple. Se ordenan todas las observaciones, de menor mayor de todos los grupos. Si al ordenarse de esta manera se da la circunstancia de que muchas de las observaciones más pequeñas (o más grandes) pertenecen un grupo sería una indicación de que los grupos presentan valores similares. Es decir, si al ordenar las observaciones de menor mayor, aparecen muy poco “mezcladas”, se rechaza la hipótesis nula. Como demostración, vamos hacerlo en el caso del ejemplo anterior:O si se prefiere de modo más visual:Aparentemente tras ordenar por la variable Diferencia de menor mayor, abundan en las primeras posiciones (rangos bajos) los individuos del grupo de Control. Es decir, estos son los que menor Diferencia han experimentado, por tanto nos hace pensar que ese grupo es diferente los otros dos. Eso es lo que obteníamos en la primera tabla del ejemplo, en la columna “P paramétrica”El equivalente la prueba posthoc de ANOVa para el test de Kruskal Wallis es el test de Dunn:La prueba de paramétrica de Kruskal-Wallis la encontramos en SPSS en el menú “Analizar - Pruebas paramétricas - k muestras independientes”.","code":"\ndf %>% arrange(Diferencia) %>% .[[\"Grupo\"]] %>% as.character()\n#>  [1] \"Control\"    \"Control\"    \"Control\"    \"Control\"   \n#>  [5] \"Control\"    \"Control\"    \"Control\"    \"Control\"   \n#>  [9] \"Técnica I\"  \"Control\"    \"Técnica I\"  \"Control\"   \n#> [13] \"Técnica II\" \"Control\"    \"Técnica I\"  \"Técnica II\"\n#> [17] \"Control\"    \"Control\"    \"Técnica II\" \"Técnica I\" \n#> [21] \"Técnica I\"  \"Técnica II\" \"Técnica II\" \"Técnica I\" \n#> [25] \"Control\"    \"Técnica II\" \"Control\"    \"Técnica I\" \n#> [29] \"Control\"    \"Técnica I\"  \"Técnica II\" \"Técnica II\"\n#> [33] \"Control\"    \"Técnica I\"  \"Control\"    \"Control\"   \n#> [37] \"Control\"    \"Técnica II\" \"Técnica II\" \"Control\"   \n#> [41] \"Técnica II\" \"Técnica I\"  \"Técnica I\"  \"Técnica II\"\n#> [45] \"Técnica II\" \"Técnica I\"  \"Técnica II\" \"Técnica II\"\n#> [49] \"Técnica II\" \"Técnica II\" \"Técnica II\" \"Técnica I\" \n#> [53] \"Técnica I\"  \"Técnica I\"  \"Técnica II\" \"Técnica II\"\n#> [57] \"Control\"    \"Técnica I\"  \"Técnica I\"  \"Técnica I\" \n#> [61] \"Técnica II\" \"Técnica I\"  \"Técnica I\"  \"Técnica II\"\n#> [65] \"Técnica I\"  \"Técnica I\"\nggplot(df %>% arrange(Diferencia) %>% mutate(Rango=rank(Diferencia)), aes(x=Rango, y=Grupo,fill=Grupo,col=Grupo))+geom_point()\nFSA::dunnTest(Diferencia ~ Grupo, data=df, method=\"bh\") \n#>               Comparison     Z P.unadj   P.adj\n#> 1    Control - Técnica I -3.84 0.00012 0.00037\n#> 2   Control - Técnica II -3.48 0.00050 0.00075\n#> 3 Técnica I - Técnica II  0.36 0.72079 0.72079"},{"path":"independencia-de-variables-categóricas.html","id":"independencia-de-variables-categóricas","chapter":"Capítulo 6 Independencia de variables categóricas","heading":"Capítulo 6 Independencia de variables categóricas","text":"Las tablas de contingencia se utilizan para examinar la relación entre dos variables categóricas, o bien explorar la distribución que posee una variable categórica entre diferentes muestras.Hay diferentes cuestiones que surgen al examinar una tabla de contingencia, y en este tema vamos tratar la cuestión de la independencia.La independencia de dos variables consiste en que la distribución de una de las variables es similar sea cual sea el nivel que examinemos de la otra. Esto se traduce en una tabla de contingencia en que las frecuencias de las filas (y las columnas) son aproximadamente proporcionales. Esto es equivalente observar que los porcentajes por columnas(o filas) son similares.La prueba de independencia \\(\\chi^2\\) (chi-cuadrado) contrasta la hipótesis de que las variables son independientes, frente la hipótesis alternativa de que una variable se distribuye de modo diferente para diversos niveles de la otra.","code":""},{"path":"independencia-de-variables-categóricas.html","id":"ejemploprioridades-en-niños-y-sexo","chapter":"Capítulo 6 Independencia de variables categóricas","heading":"Ejemplo:Prioridades en niños y sexo","text":"Vamos usar la base de datos coolKids.sav donde se recogen las respuestas de unos escolares de 10 12 años los que se les preguntó, entre otras cosas, qué daban más prioridad de entre tres posibilidades: Tener buenas notas, destacar en los deportes o ser popular entre los compañeros. Examinemos las primeras líneas de la base de datos:Vamos estudiar por sexos si por sexo hay diferencias en la distribución de las metas:La significación que se observa en la tabla (p), es la de la prueba Chi-cuadrado. Nos indica que las diferencias en porcentajes que se aprecian en las columnas de niños y niñas van más allá de lo que se esperaría que ocurriese por puro azar.Con un poco de atención se observa porcentajes similares de niños y niñas en cuanto la importancia que tienen para ellos las notas. Donde más diferencia se observa entre los sexos es en la preferencia que muestran muchos chicos por los deportes y muchas chicas por la popularidad.Gráficamente podríamos mostrarlo con diagramas de barras apiladas:Otra forma interesante de mostrarlo consiste en utilizar gráficos de mosaicos, donde se utiliza un área para cada combinación de las dos variables, proporcional las frecuencias observadas. Observe que son ligeramente más anchos los mosaicos de las mujeres que los de los varones. Esto refleja que en el estudio participa una cantida liéramente mayor de hombres que de mujeres.Usando SPSS podemos conseguir tanto las frecuencias como los porcentajes y resultado de la prueba chi-cuadrado en el menú:\n“Analizar - Estadísticos descriptivos - Tablas de contingencia…”. Allí situamos en columnas la variable Sexo, y en filas ponemos Metas. En el botón Casillas marcamos los porcentajes por columnas. En el botón “Estadísticos…” para marcar seleccionamos la prueba chi-cuadrado de Pearson.Podríamos afinar más nuestro estudio y ver si estas diferencias entre niños y niñas se presentan también según el Entorno (urbano, suburbano o rural) de los niños. Para ello podríamos realizar una prueba chi-cuadrado en cada entorno:Para hacer lo mismo con SPSS, en el mismo diálogo donde hacíamos tablas cruzadas, pasamos la variable Entornos al campos Capas.","code":"\ndf=read_sav(\"datos/coolKids.sav\", user_na=FALSE) %>% haven::as_factor() \ndf %>% head() %>% knitr::kable(booktabs=T)\ndf %>% generaTablaChi2PorGrupo(vGrupo = \"Sexo\",vCuali = \"Metas\") %>% \n  select(1:5) %>% \n  knitr::kable(booktabs=T) %>%collapse_rows(1:5,valign = \"top\")\ndftmp=df %>% select(Sexo,Metas)  %>% mutate(cuenta=1) %>% group_by(Sexo,Metas) %>% tally() %>% mutate(fraccion=n/sum(n))\nggplot(dftmp, aes(fill=Metas, y=fraccion, x=Sexo)) + \n    geom_bar( stat=\"identity\", position=\"fill\")\nggplot(data = df) +\n   geom_mosaic(aes(x = product(Metas,Sexo), fill=Metas), na.rm=TRUE) +\n   labs(x=\"Sexo\", y=\"Metas\",title='Metas por Sexo')\ndf %>% generaTablaChi2PorGrupo_Tiempo(vGrupo = \"Sexo\", vCuali = \"Metas\", vTiempo = \"Entorno\") %>% \n  select(2,3,4,5,6) %>% \n  knitr::kable(booktabs=T) %>%collapse_rows(1:5,valign = \"top\")\nggplot(data = df) +\n   geom_mosaic(aes(x = product(Metas,Sexo), fill=Metas), na.rm=TRUE) +\n   labs(x=\"Sexo\", y=\"Metas\",title='Metas por Sexo') + facet_grid(Entorno~.)"},{"path":"independencia-de-variables-categóricas.html","id":"limitaciones-de-la-prueba-de-independencia","chapter":"Capítulo 6 Independencia de variables categóricas","heading":"6.1 Limitaciones de la prueba de independencia","text":"El contraste de independencia tiene muy pocas limitaciones, aunque es conveniente hacer algunas observaciones:Para contrastar la independencia se suele usar el estadístico chi-cuadrado de Pearson. Su cálculo se basa en calcular la diferencia entre las observaciones observadas para cada par de modalidades de las variables (casillas), y las que serían de esperar en caso de que se satisficiese la condición de independencia. Para que se pueda considerar correcta la significación calculada por el estadístico chi-cuadrado de Pearson, se debe cumplir que las frecuencias esperadas sean muy pequeñas (inferiores 5) más que en unas pocas casillas. Si es en muchas las casillas donde esto ocurre (más del 20% por ejemplo) se debe usar una prueba que incluya aproximaciones, como la prueba exacta de Fisher. Esta la ofrece cualquier programa como opción cuando se hace este tipo de contrastes.Para contrastar la independencia se suele usar el estadístico chi-cuadrado de Pearson. Su cálculo se basa en calcular la diferencia entre las observaciones observadas para cada par de modalidades de las variables (casillas), y las que serían de esperar en caso de que se satisficiese la condición de independencia. Para que se pueda considerar correcta la significación calculada por el estadístico chi-cuadrado de Pearson, se debe cumplir que las frecuencias esperadas sean muy pequeñas (inferiores 5) más que en unas pocas casillas. Si es en muchas las casillas donde esto ocurre (más del 20% por ejemplo) se debe usar una prueba que incluya aproximaciones, como la prueba exacta de Fisher. Esta la ofrece cualquier programa como opción cuando se hace este tipo de contrastes.Si las muestras son muy grandes, la prueba de independencia dará resultados significativos incluso donde, posiblemente, consideremos que las diferencias sean en realidad clínicamente interesantes. Es conveniente una inspección visual para confirmar si las diferencias observadas por filas (o columnas, como prefiramos), nos parecen de interés.Si las muestras son muy grandes, la prueba de independencia dará resultados significativos incluso donde, posiblemente, consideremos que las diferencias sean en realidad clínicamente interesantes. Es conveniente una inspección visual para confirmar si las diferencias observadas por filas (o columnas, como prefiramos), nos parecen de interés.Si las variables poseen muchos niveles posiblemente la prueba resulte de mucho interés, ya que es lógico esperar que se encuentren diferencias. Eso ocurre si por ejemplo una de las variables es numérica y hemos agrupado los posibles valores en una cantidad adecuada de intervalos.Si las variables poseen muchos niveles posiblemente la prueba resulte de mucho interés, ya que es lógico esperar que se encuentren diferencias. Eso ocurre si por ejemplo una de las variables es numérica y hemos agrupado los posibles valores en una cantidad adecuada de intervalos.Si una de las variables es numérica u ordinal, posiblemente queramos hacer algo más que contrastar la simple independencia. Después de todo, esto es tan informativo como saber que en cierto grupo los valores son significativamente mayores que en otro. Lo aconsejable es usar pruebas de tipo t-student, anova o contrastes paramétricos comos los que se tratan en otros temas.Si una de las variables es numérica u ordinal, posiblemente queramos hacer algo más que contrastar la simple independencia. Después de todo, esto es tan informativo como saber que en cierto grupo los valores son significativamente mayores que en otro. Lo aconsejable es usar pruebas de tipo t-student, anova o contrastes paramétricos comos los que se tratan en otros temas.El contraste de chi-cuadrado sirve para contrastar la independencia. hay que considerarla como una medida de la asociación entre variables. Si buscamos estudiar la asociación de variables tenemos otros métodos nuestra disposición como regresión logística que trataremos más adelante.El contraste de chi-cuadrado sirve para contrastar la independencia. hay que considerarla como una medida de la asociación entre variables. Si buscamos estudiar la asociación de variables tenemos otros métodos nuestra disposición como regresión logística que trataremos más adelante.","code":""},{"path":"regresión-lineal-múltiple.html","id":"regresión-lineal-múltiple","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"Capítulo 7 Regresión lineal múltiple","text":"Utilizamos regresión lineal múltiple cuando estudiamos la posible relación entre varias variables independientes (predictoras o explicativas) y otra variable dependiente (criterio, explicada, respuesta).Por ejemplo, podemos estar interesados en saber cómo influyen en la presión arterial sistólica de un paciente el peso, la edad y el sexo, donde la variable sexo es una variable dicotómica (o indicadora), codificada como 0 para las mujeres y 1 para los hombres.La técnica de regresión múltiple se usa frecuentemente en investigación. Se aplica al caso en que la variable respuesta es de tipo numérico. Cuando la respuesta es de tipo dicotómico (muere/vive, enferma/enferma), usamos otra técnica denominada regresión logística y que tratamos en un capítulo posterior.","code":""},{"path":"regresión-lineal-múltiple.html","id":"ejemplo-2","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"Ejemplo","text":"Descargamos la base de datos que hemos usado en ocasiones anteriores centroSalud-transversal.sav y exploramos algunas las primeras líneas de las variables sexo, edad, peso y pas:Estudiamos el modelo de regresión lineal donde intentamos estudiar la inflencia de sexo, edad, peso en pas:Esta salida de se puede interpretar como que el mejor modelo lineal para explicar la pas partir de las variables elegidas es:\\[ pas = 100.5 +4.9 sexoMujer + 0.41 * edad + 0.13*peso \\]\nO como resulta normalmente más interesante de interpretar:igualdad de peso y edad, una mujer suele tener una pas 4.9 mmHG más alta que un hombre.igualdad de peso y edad, una mujer suele tener una pas 4.9 mmHG más alta que un hombre.igualdad de sexo y peso, los individuos aumentan 0.41 mmHg de pas cada año que cumplen.igualdad de sexo y peso, los individuos aumentan 0.41 mmHg de pas cada año que cumplen.igualdad de sexo y edad, la pas sube 0.13 mmHg por cada kg de peso que aumentan.igualdad de sexo y edad, la pas sube 0.13 mmHg por cada kg de peso que aumentan.Cada una de esas estimaciones se muestra con su respectivo error estándar y significación. La significación del sexo por ejemplo, se interpreta como que igualdad de peso y edad, las mujeres muestran unos valores medios de pas que son significativamente diferentes de cero.También es habitual el mostrar, además de los coeficientes ajustados (multivariable), los coeficientes crudos (univariable), que son los que se obtendrían al analizar de forma separada la variable dependiente con cada una de las independientes. Presentar las tablas así permite identificar variables confusoras.Para obtener los resultados anteriores en SPSS usaríamos el menú -Analizar - Regresión - Lineales,\ncolocaríamos en dependientes las variable pas, y en independientes el resto.","code":"\ndf=read_sav(\"datos/centroSalud-transversal.sav\", user_na=FALSE) %>% haven::as_factor()\ndf %>% select(sexo,edad,peso,pas) %>% head()  %>% knitr::kable(booktabs=T)\ndf %>% lm(pas ~ sexo+edad+peso, data=.) %>% model_parameters(show.se = TRUE,summary = TRUE) %>% print_html()\ntabla=df %>% finalfit(dependent = \"pas\", explanatory=c(\"sexo\",\"edad\",\"peso\"),metrics = TRUE)  "},{"path":"regresión-lineal-múltiple.html","id":"aplicaciones-de-la-regresión-múltiple","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.1 Aplicaciones de la regresión múltiple","text":"Es cierto que la regresión múltiple se utiliza para la predicción de respuestas partir de variables explicativas. Pero es ésta realmente la aplicación que se le suele dar en investigación. Los usos que con mayor frecuencia encontraremos en las publicaciones son los siguientes:Identificación de variables explicativas. Nos ayuda crear un modelo donde se seleccionen las variables que puedan influir en la respuesta, descartando aquellas que aporten información.Identificación de variables explicativas. Nos ayuda crear un modelo donde se seleccionen las variables que puedan influir en la respuesta, descartando aquellas que aporten información.Detección de interacciones entre variables independientes que afectan la variable respuesta. Un ejemplo de interacción clásico es el de estudiar la respuesta de un paciente al alcohol y un barbitúrico, y observar que cuando se ingieren ambos, el efecto es mucho mayor del esperado como suma de los dos.Detección de interacciones entre variables independientes que afectan la variable respuesta. Un ejemplo de interacción clásico es el de estudiar la respuesta de un paciente al alcohol y un barbitúrico, y observar que cuando se ingieren ambos, el efecto es mucho mayor del esperado como suma de los dos.Identificación de variables confusoras. Es un problema difícil el de su detección, pero de interés en investigación experimental, ya que el investigador frecuentemente tiene control sobre las variables independientes.Identificación de variables confusoras. Es un problema difícil el de su detección, pero de interés en investigación experimental, ya que el investigador frecuentemente tiene control sobre las variables independientes.","code":""},{"path":"regresión-lineal-múltiple.html","id":"requisitos-y-limitaciones","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.2 Requisitos y limitaciones","text":"Hay ciertos requerimientos necesarios para poder utilizar la técnica de regresión múltiple:Linealidad: Se supone que la variable respuesta depende linealmente de las variables explicativas. Si la respuesta aparenta ser lineal, debemos introducir en el modelo componentes lineales (como incluir transformaciones lineales de las variables independientes en el modelo). Otro tipo de respuesta lineal es la interacción. Para ello se ha de incluir en el modelo términos de interacción, que equivalen introducir nuevas variables explicativas que en realidad son el producto de dos o más de las independientes.Linealidad: Se supone que la variable respuesta depende linealmente de las variables explicativas. Si la respuesta aparenta ser lineal, debemos introducir en el modelo componentes lineales (como incluir transformaciones lineales de las variables independientes en el modelo). Otro tipo de respuesta lineal es la interacción. Para ello se ha de incluir en el modelo términos de interacción, que equivalen introducir nuevas variables explicativas que en realidad son el producto de dos o más de las independientes.Normalidad y equidistribución de los residuos: Se llaman residuos las diferencias entre los valores calculados por el modelo y los realmente observados en la variable dependiente. Para tener un buen modelo de regresión es suficiente con que los residuos sean pequeños. La validez del modelo requiere, en teoríam que los mismos se distribuyan de modo normal y con la misma dispersión para cada combinación de valores de las variables independientes. Por supuesto, esta condición en la práctica es inverificable, puesto que para cada combinación de variables independientes tendremos normalmente ninguna o una respuesta. Lo que se suele hacer es examinar una serie de gráficos de residuos que nos hagan sospechar. Por ejemplo si los residuos aumentan al aumentar la respuesta, o vemos que aparecen tendencias,… Es decir, hay una serie de reglas heurísticas que nos ayudan decidir si aceptar o el modelo de regresión, pero están basadas en contrastes de hipótesis como hemos usado hasta ahora. Es la experiencia del investigador observando residuos la que le decide usarlo o .Normalidad y equidistribución de los residuos: Se llaman residuos las diferencias entre los valores calculados por el modelo y los realmente observados en la variable dependiente. Para tener un buen modelo de regresión es suficiente con que los residuos sean pequeños. La validez del modelo requiere, en teoríam que los mismos se distribuyan de modo normal y con la misma dispersión para cada combinación de valores de las variables independientes. Por supuesto, esta condición en la práctica es inverificable, puesto que para cada combinación de variables independientes tendremos normalmente ninguna o una respuesta. Lo que se suele hacer es examinar una serie de gráficos de residuos que nos hagan sospechar. Por ejemplo si los residuos aumentan al aumentar la respuesta, o vemos que aparecen tendencias,… Es decir, hay una serie de reglas heurísticas que nos ayudan decidir si aceptar o el modelo de regresión, pero están basadas en contrastes de hipótesis como hemos usado hasta ahora. Es la experiencia del investigador observando residuos la que le decide usarlo o .Número de variables independientes: Podemos estar tentados en incluir en el modelo cualquier cosa que tengamos en una base de datos, con la esperanza de que cuantas más variables incluyamos, más posibilidades hay de que “suene la flauta”. Si nos aborda esta tentación, hemos de recordar que corremos el riesgo de cometer error de tipo . Otra razón es que si esperamos ajustar unas pocas observaciones usando muchas variables, muy probablemente consigamos una aproximación muy artificial, y además muy sensible los valores observados. La inclusión de una nueva observación puede cambiar completamente el valor de los coeficientes del modelo. Una regla que se suele recomendar es la de incluir al menos 20 observaciones por cada variable independiente que estimemos priori interesantes en el modelo.Número de variables independientes: Podemos estar tentados en incluir en el modelo cualquier cosa que tengamos en una base de datos, con la esperanza de que cuantas más variables incluyamos, más posibilidades hay de que “suene la flauta”. Si nos aborda esta tentación, hemos de recordar que corremos el riesgo de cometer error de tipo . Otra razón es que si esperamos ajustar unas pocas observaciones usando muchas variables, muy probablemente consigamos una aproximación muy artificial, y además muy sensible los valores observados. La inclusión de una nueva observación puede cambiar completamente el valor de los coeficientes del modelo. Una regla que se suele recomendar es la de incluir al menos 20 observaciones por cada variable independiente que estimemos priori interesantes en el modelo.Colinealidad: Si dos variables independientes están estrechamente relacionadas (consumo de refrescos y temperatura ambiente por ejemplo) y ambas son incluidas en un modelo, muy posiblemente ninguna de las dos sea considerada significativa, aunque si hubiésemos incluido sólo una de ellas, sí. Hay diferentes técnicas para detectar la colinealidad pero que requieren profundizar en documentos más sofisticados. Aquí vamos indicar una técnica muy simple: examinar los coeficientes del modelo para ver si se vuelven inestables al introducir una nueva variable. Si es así posiblemente hay colinealidad entre la nueva variable y las anteriores.Colinealidad: Si dos variables independientes están estrechamente relacionadas (consumo de refrescos y temperatura ambiente por ejemplo) y ambas son incluidas en un modelo, muy posiblemente ninguna de las dos sea considerada significativa, aunque si hubiésemos incluido sólo una de ellas, sí. Hay diferentes técnicas para detectar la colinealidad pero que requieren profundizar en documentos más sofisticados. Aquí vamos indicar una técnica muy simple: examinar los coeficientes del modelo para ver si se vuelven inestables al introducir una nueva variable. Si es así posiblemente hay colinealidad entre la nueva variable y las anteriores.Observaciones anómalas: Está muy relacionada con la cuestión de los residuos, pero merece destacarlo aparte. Debemos poner especial cuidado en identificarlas (y descartarlas si procede), pues tienen gran influencia en el resultado. veces, son sólo errores en la entrada de datos, pero de gran consecuencia en el análisis. Hay técnicas de regresión robustas que permiten minimizar su efecto.Observaciones anómalas: Está muy relacionada con la cuestión de los residuos, pero merece destacarlo aparte. Debemos poner especial cuidado en identificarlas (y descartarlas si procede), pues tienen gran influencia en el resultado. veces, son sólo errores en la entrada de datos, pero de gran consecuencia en el análisis. Hay técnicas de regresión robustas que permiten minimizar su efecto.","code":""},{"path":"regresión-lineal-múltiple.html","id":"variables-numéricas-e-indicadoras-dummy","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.3 Variables numéricas e indicadoras (dummy)","text":"Un modelo de regresión lineal tiene el aspecto:\\[ Y = b_0 +b_1 X_1 + b_2 X_2+...+b_n X_n \\]\ndonde:Y es la variable dependienteY es la variable dependienteLos términos \\(X_i\\) representan las variables independientes o explicativasLos términos \\(X_i\\) representan las variables independientes o explicativasLos coeficientes del modelo, \\(b_i\\) son calculados por el programa estadístico, de mode que se minimicen los residuos.Los coeficientes del modelo, \\(b_i\\) son calculados por el programa estadístico, de mode que se minimicen los residuos.Esencialmente cuando obtengamos para los coeficientes valores “compatibles” con cero (significativos), la variable asociada se elimina del modelo, y en otro caso se considera la variable asociada de interés. Esta regla hay que aplicarla ciegamente. Si por ejemplo la variable con coeficiente significativo se observa que es confusora o laliteratura nos indica que debe ser tenida en cuenta en el análisis, debemos considerarla como parte del modelo, bien explícitamente o estratificando la muestra según los diferentes valores de la misma.Está claro que para ajustar el modelo la variable respuesta debe ser numérica. Sin embargo, aunque pueda parecer extraño tienen por qué serlo las variables explicativas. Aunque requiere un artificio, podemos utilizar predictores categóricos mediante la introducción de variables indicadoras (también denominadas mudas o dummy)Si una variable es dicotómica, puede ser codificada como 0 ó 1. Así si estudiamos la explicación del peso de una persona como función de su altura y su sexo, un modelo como:\n\\[Peso =-100 + 1 \\times Altura - 5\\times Sexo\\]donde se ha codificado con Sexo=0 los hombres y Sexo=1 las mujeres, puede ser interpretado como que las mujeres, igualdad de altura, pesan de media 5 Kg menos que los hombres. El coeficiente 1 de la altura, se interpreta como que por cada diferencia de altura de un centímetro en personas que tienen el resto de variables independientes iguales (mismo sexo), el peso aumenta, por término medio, en 1 kg.","code":""},{"path":"regresión-lineal-múltiple.html","id":"ejemplo-3","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.3.0.1 Ejemplo","text":"La base de datos peso_altura_sexo.sav contiene datos simulados que han sido contruidos para dar un modelo de regresión similar al descrito anteriormente. Una analisis de regresión nos ofrecería por resultado:","code":"\ndf=read_sav(\"datos/peso_altura_sexo.sav\") %>% haven::as_factor()\ndf %>% lm(Peso ~ Sexo+ Altura, data=.) %>% model_parameters(show.se = TRUE,summary = TRUE) %>% print_html()"},{"path":"regresión-lineal-múltiple.html","id":"ejemplo-regresión-lineal-generaliza-a-las-pruebas-t-student-para-dos-muestras-y-anova-de-un-factor","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"Ejemplo: Regresión lineal generaliza a las pruebas t-student para dos muestras, y ANOVA de un factor","text":"Observe que el modelo de regresión múltiple generaliza otras técnicas estadísticas que conocemos estas alturas como el modelo t-student para 2 muestras independientes o ANOVA de un factor. Un contraste de dos medias independientes puede resolverse con una regresión de la variable respuesta en función de una variable indicadora que identifica sendas muestras. Un modelo ANOVA de un factor se puede expresar usando variables indicadoras suficientes para codificar el grupo. Para poder interpretar cómodamente los resultados es importante que los grupos sean equilibrados (cada muestra debe tener un número similar de elementos).","code":""},{"path":"regresión-lineal-múltiple.html","id":"bondad-de-ajuste","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.4 Bondad de ajuste","text":"Al ajustar un modelo de regresión es necesario mostar un término denominado \\(R^2\\), bondad de ajuste, o porcentaje de variabilidad exolicado por el modelo o bien, coeficiente de determinación. Se interpreta del siguiente modo: La variable respuesta presenta cierta variabilidad (incertidumbre), pero cuando se conoce el valor de las variables independientes, dicha incertidumbre disminuye. El término R cuadrado es una cantidad que puede interpretarse como un factor (porcentaje) de reducción de la incertidumbre cuando son conocidas las variables independientes. Cuanto más se acerque uno, más poder explicativo tendrá el modelo. Pero esto esconde una trampa. Cada vez que introducimos una nueva variable independiente en el modelo, R cuadrado puede hacer otra cosa que aumentar. Si introducimos un número artificialmente grande de ellas, podremos llegar acercarla uno tanto como queramos.Para practicar con \\(R^2\\), en un modelo de regresión siga este enlace:Los programas estadísticos nos muestran un término R cuadrado corregida, que puede interpretarse como una corrección de honestidad. Nos castigará disminuyendo cuando introduzcamos variables innecesarias. Si al ir complicando el modelo este término aumenta una cantidad “razonable”, podemos considerarlo posiblemente una variable de interés, pero si disminuye, deberíamos pensar dos veces si nos merece la pena la complejidad del modelo para tan poco beneficio.","code":""},{"path":"regresión-lineal-múltiple.html","id":"matriz-de-correlaciones","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.5 Matriz de correlaciones","text":"La matriz de correlaciones nos ayuda identificar correlaciones lineales entre pares de variables. Encontrar correlaciones lineales entre la variable dependiente y cualquiera de las independientes es siempre de interés. Pero es una mala señal la alta correlación entre variables independientes y tal vez nos haga excluir alguna del modelo, por contener información muy similar.La matriz de correlaciones está formada por todos los coeficientes de correlación lineal de Pearson para cada par de variables. Los mismos son cantidades que pueden tomar valores comprendidos entre -1 y +1. Cuanto más extremo sea el coeficiente, mejor asociación lineal existe entre el par de variables. Cuando es cercano cero, . El signo positivo del coeficiente nos indica que la asociación es directa (cuando una variable crece la otra también). Un valor negativo indica que la relación es inversa (cuando una crece, la otra decrece).","code":""},{"path":"regresión-lineal-múltiple.html","id":"variables-confusoras","chapter":"Capítulo 7 Regresión lineal múltiple","heading":"7.6 Variables confusoras","text":"Dos variables o más variables están confundidas cuando sus efectos sobre la variable dependiente pueden ser separados. Dicho de otra forma, una variable es confusora cuando estando relacionada con alguna variable independiente, su vez afecta la dependiente.Cuando se identifica una variable que está confundida con alguna de las variables independientes significativa, es necesario dejarla formar parte del modelo, tenga o mucha significación. Las variables confusoras pueden ser ignoradas.Puede ayudarnos identificar una variable confusora el encontrarnos con un modelo en el que una variable independiente parece tener cierta influencia significativa del signo que sea en la variable respuesta, pero al incluir una nueva variable previamente ignorada (variable confusora) se observa que la primera tiene una influencia claramente diferente (incluso con el signo cambiado y aún significativa). El simple hecho de que lo anteriormente mencionado ocurra, es la prueba de que ambas variables estén confundidas, pero nos invita reflexionar.###Ejemplo {-}Vamos retomar un ejemplo que desarrollamos en el capítulo donde tratábamos la técnica ANOVA, para usar con él un modelo de regresión múltiple: lectura-anova.sav.Recordamos que se realizó un experimento para comparar tres métodos de aprendizaje de lectura. Se asignó aleatoriamente los estudiantes cada uno de los tres métodos. Cada método fue probado con 22 estudiantes. Se evaluó mediante diferentes pruebas la capacidad de comprensión de los estudiantes, antes y después de recibir la instrucción. Por tanto tenemos 3 variables numéricas que son la capacidad al inicio del experimento, al final, y la que resulta más interesante, la diferencia.Podemos usar la pareja de variables gr1 y gr2 como indicadoras de pertenencia al Grupo. Un análisis de regresión lineal nos ofrece el mismo resultado que la prueba ANOVA. Compruebe la diferencia existente entre la salida del modelo de regresión lineal con las variables indicadoras:y la salida del modelo ANOVA de un factor:Realmente las dos formas de escribir el análisis han hecho exactamente lo mismo internamente.Podemos refinar el estudio del siguiente modo:Variable dependiente: La diferencia entre la capacidad “después” y “antes”.Variable dependiente: La diferencia entre la capacidad “después” y “antes”.Variables explicativas:Variables explicativas:La capacidad al inicio del experimento. Posiblemente los estudiantes con mejor capacidad inicial sacaron menos provecho que el resto.La capacidad al inicio del experimento. Posiblemente los estudiantes con mejor capacidad inicial sacaron menos provecho que el resto.La técnica utilizada. Como es una variable categórica que se utiliza para identificar la muestra y tiene tres categorías podemos codificarla usando dos variables indicadoras (gr1 y gr2, o lo que será equivalente, considerar solo la variable Grupo).La técnica utilizada. Como es una variable categórica que se utiliza para identificar la muestra y tiene tres categorías podemos codificarla usando dos variables indicadoras (gr1 y gr2, o lo que será equivalente, considerar solo la variable Grupo).El resultado del análisis sería:Obsérvese que al mejorar \\(R^2\\) hemos obtenido un mejor modelo explicativo, independientemente de que nos interese o la significación de la nueva variable introducida. En ocasiones es útil mostrar una comparativa entre varios modelos. La forma que se suele dar la tabla para compararlos es la siguiente:Podríamos interpretar que el segundo modelo es preferible al primero. Tiene solo mejor \\(R^2\\) (cuanto más alto, mejor), si que otro indicador, AIC (Akaike Information Criteria), tiene un valor más bajo (cuanto más bajo, mejor la hora de decidir entre múltiples modelos).Para hacer lo mismo en SPSS, se puede utilizar como hemos hecho aquí la variable Grupo, pero sí las dos indicadoras (gr1 y gr2) que contienen la misma información. Para ello vamos la opción de menú “Analizar – Regresión - Lineales”, y situamos en su lugar las variables independientes y la dependiente.","code":"\ndf=read_sav(\"datos/lectura-anova.sav\", user_na=FALSE) %>% haven::as_factor() \ndf %>% head()  %>% knitr::kable(booktabs=T)\ndf %>% lm(Diferencia ~ gr1 + gr2, data=.) %>% model_parameters(show.se = TRUE,summary = TRUE) %>% print_html()\nmodelo1=df %>% lm(Diferencia ~ Grupo, data=.) \nmodelo1 %>% model_parameters(show.se = TRUE,summary = TRUE) %>% print_html()\nmodelo2=df %>% lm(Diferencia ~ Grupo+Antes, data=.) \n\nmodelo2%>% model_parameters(show.se = TRUE,summary = TRUE) %>% print_html()\ntab_model(modelo1, modelo2,show.se = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% asis_output()"},{"path":"regresión-logística-binaria.html","id":"regresión-logística-binaria","chapter":"Capítulo 8 Regresión logística binaria","heading":"Capítulo 8 Regresión logística binaria","text":"En ocasiones estamos interesados en conocer la influencia que una serie de variables tienen en una variable respuesta. Cuando la misma era numérica una herramienta que estaba nuestra disposición era la regresión múltiple. Pero, ¿qué podemos hacer cuando la respuesta es dicotómica? Por ejemplo, ¿qué podemos hacer si la respuesta observada es el desarrollo o de una enfermedad?Este tipo de sitiaciones aparecen de manera natural en investigaciones médicas. Citemos unos cuantos ejemplos:Se cree que la apnea del sueño obstructiva es un factor de riesgo para la hipertensión arterial. Podríamos reformularlo en lenguaje estadístico como que la variable independiente padecer apnea obstructiva del sueño, está asociado (es factor de riesgo) para el la ocurrencia del evento hipertensión arterial.Se cree que la apnea del sueño obstructiva es un factor de riesgo para la hipertensión arterial. Podríamos reformularlo en lenguaje estadístico como que la variable independiente padecer apnea obstructiva del sueño, está asociado (es factor de riesgo) para el la ocurrencia del evento hipertensión arterial.Se cree que la intervención por laparoscopia para tratar la hernia de hiato ofrece menor riesgo de complicaciones postoperatorias que otra técnica tradicional. La variable respuesta sería padecer complicaciones (sí o ), y la variable independiente sería el tipo de operación.Se cree que la intervención por laparoscopia para tratar la hernia de hiato ofrece menor riesgo de complicaciones postoperatorias que otra técnica tradicional. La variable respuesta sería padecer complicaciones (sí o ), y la variable independiente sería el tipo de operación.Se cree que fumar es un factor de riesgo para la muerte fetal tardía. Esto se podría formular de varias maneras:Se cree que fumar es un factor de riesgo para la muerte fetal tardía. Esto se podría formular de varias maneras:Podemos considerar una variable independiente que es la “madre fuma” (sí o ) y una variable respuesta (dependiente) que es el “feto muere” (sí o ). Nos interesará evaluar cuánto aumenta el riesgo de que se pruduzca el evento de interés (muerte del feto) cuando está presente el factor de riesgo (la madre fuma).Podemos considerar una variable independiente que es la “madre fuma” (sí o ) y una variable respuesta (dependiente) que es el “feto muere” (sí o ). Nos interesará evaluar cuánto aumenta el riesgo de que se pruduzca el evento de interés (muerte del feto) cuando está presente el factor de riesgo (la madre fuma).Otra aproximación podría ser considerar como variable numérica “número medio de cigarrillos que fuma la madre”. En este caso nos puede interesar estimar cuánto aumenta el riesgo de muerte del feto, por cada cigarrilo adicional que fuma la madre diariamente.Otra aproximación podría ser considerar como variable numérica “número medio de cigarrillos que fuma la madre”. En este caso nos puede interesar estimar cuánto aumenta el riesgo de muerte del feto, por cada cigarrilo adicional que fuma la madre diariamente.Si el aumento del riesgo (se evalúe como se evalúe) parece tener una tendencia constante con el número de cigarrillos, sino que mas bién se puede dividir las madres en tres categorías “fuma”, “Fuma poco”, “Fuma mucho”, nos interesará evaluar cómo aumenta el riesgo en las dos últimas categorías con respecto las madres del primer grupo (grupo de control, o de referencia).Si el aumento del riesgo (se evalúe como se evalúe) parece tener una tendencia constante con el número de cigarrillos, sino que mas bién se puede dividir las madres en tres categorías “fuma”, “Fuma poco”, “Fuma mucho”, nos interesará evaluar cómo aumenta el riesgo en las dos últimas categorías con respecto las madres del primer grupo (grupo de control, o de referencia).El modelo de regresión logística binaria es muy útil para abordar este tipo de cuestiones bajo la condición de que hayamos tenido en cuenta al realizar el estudio todas las variables importantes para explicar la variable respuesta, y que hagamos el estudio con una muestra suficientemente numerosa y bien distribuida.Antes de pasar directamente al modelo de regresión logística vamos refrescar rápidamente una serie de conceptos relacionados con la comparaciones de riesgos.","code":""},{"path":"regresión-logística-binaria.html","id":"riesgo-oportunidad-riesgo-relativo-y-odds-ratio","chapter":"Capítulo 8 Regresión logística binaria","heading":"8.1 Riesgo, Oportunidad, Riesgo Relativo y Odds Ratio","text":"En medicina es frecuente encontrar todos estos términos. Vamos repasarlos pues será necesario manejarlos con cierta soltura en el resto del capítulo.Aquí tiene un simulador para practicar con Odds RatioEn 1 de cada 200 nacimientos ocurre un parto gemelar. Por tanto la probabilidad o riesgo de que elegido un embarazo al azar éste de lugar gemelos es de \\(R_1=1/200\\). Esto es simplemente, el número de casos en que el evento ocurre dividido por el total de casos.Hay otra forma de decir lo mismo, que seguramente ha sido tomada del lenguaje usado por los anglosajones en las apuestas. Consiste en la oportunidad/ventaja (del inglés odds). Podemos decir que de 200 partos, 1 es gemelar y 199 lo son. Las apuestas están 1 199. Se denomina oportunidad la cantidad \\(O_1=1/199\\), es decir, el número de casos en los que el evento ocurre dividido por el número de casos en que ocurre. En el fondo es sólo una manera anglosajona de decir lo mismo que con probabilidades. Hasta aquí hay nada especial.Compliquemos la cosa un poquito, introduciendo un factor de riesgo. Se observó que entre las mujeres que han tomado ácido fólico para disminuir la probabilidad de espina bífida en sus hijos, ocurrió algo esperado: 3 de cada 200 partos eran gemelares.\nEsto corresponde un riesgo de \\(R_2=3/200\\), o si lo preferimos, una oportunidad (odds) de 3 197, \\(O_2=3/197\\).¿Cómo podemos expresar numéricamente el aumento del riesgo de embarazo gemelar? Hay dos maneras. Una de ellas es más fácil de entender, y la otra tiene mejores propiedades matemáticas.Riesgo Relativo (RR): Este es el más simple de entender. Claramente el riesgo ha aumentado por 3, lo que corresponde un Riesgo Relativo (RR) que es el cociente entre el riesgo de los embarazos expuestos al ácido fólico (factor de riesgo) y los que han sido expuestos,\n\\[RR =  \\frac{R_2}{R_1} =  \\frac{3/200}{1/200} = 3\\]Riesgo Relativo (RR): Este es el más simple de entender. Claramente el riesgo ha aumentado por 3, lo que corresponde un Riesgo Relativo (RR) que es el cociente entre el riesgo de los embarazos expuestos al ácido fólico (factor de riesgo) y los que han sido expuestos,\n\\[RR =  \\frac{R_2}{R_1} =  \\frac{3/200}{1/200} = 3\\]Odds Ratio (): En español se traduce veces en textos académicos como Oportunidad Relativa o Razón de Ventajas, aunque en las publicaciones aparece más frecuentemente con el término inglés. Es parecido al RR, pero usando oportunidades (odds). Es el cociente entre la oportunidad de los embarazos expuestos al ácido fólico (factor de riesgo) y los que han sido expuestos,Odds Ratio (): En español se traduce veces en textos académicos como Oportunidad Relativa o Razón de Ventajas, aunque en las publicaciones aparece más frecuentemente con el término inglés. Es parecido al RR, pero usando oportunidades (odds). Es el cociente entre la oportunidad de los embarazos expuestos al ácido fólico (factor de riesgo) y los que han sido expuestos,\\[= \\frac{O_2}{O_1} =  \\frac{3/197}{1/199} = 3.03\\]Desde luego es tan fácil de interpretar una como lo es un RR, aunque en este caso poseen valores muy similares. Esto ocurre siempre que la probabilidad de que ocurra un evento sea cercana cero, como en el caso de un embarazo gemelar. Cuando las probabilidades del evento son cercanas cero, ambas cantidades son iguales y hay que tener cuidado con confundirlas.Tanto para como RR, tenemos las siguientes propiedades:Un valor de \\(=1\\) se interpreta como que hay tal factor de riesgo, ya que la oportunidad para los expuestos es la misma que para los expuestos.Un valor de \\(=1\\) se interpreta como que hay tal factor de riesgo, ya que la oportunidad para los expuestos es la misma que para los expuestos.En epidemiología es frecuente intentar localizar factores dañinos. Eso corresponde buscar valores de \\(\\gt 1\\). Se interpreta como qué se ha localizado un factor de riesgo, pues es mayor la oportunidad de que ocurra el evento los expuestos al factor que los controles.En epidemiología es frecuente intentar localizar factores dañinos. Eso corresponde buscar valores de \\(\\gt 1\\). Se interpreta como qué se ha localizado un factor de riesgo, pues es mayor la oportunidad de que ocurra el evento los expuestos al factor que los controles.En los ensayos clínicos, se persigue encontrar tratamientos que reduzcan la frecuencia de un evento (por ejemplo, la muerte del enfermo). En este caso se buscan valores \\(\\lt 1\\). Es decir, que sea menor la oportunidad de que ocurra el evento en los individuos expuestos al tratamiento que en los controles.En los ensayos clínicos, se persigue encontrar tratamientos que reduzcan la frecuencia de un evento (por ejemplo, la muerte del enfermo). En este caso se buscan valores \\(\\lt 1\\). Es decir, que sea menor la oportunidad de que ocurra el evento en los individuos expuestos al tratamiento que en los controles.Por otro lado \\(\\) tiene muy buenas propiedades matemáticas:\\(\\) toma valores entre cero e infinito. Esto lo hace muy adecuado para ser modelado matemáticamente. Sobre todo si tomamos su logaritmo, ya que en ese caso cualquier valor es posible. El modelo que consideraremos posteriormente será el de regresión logística.\\(\\) toma valores entre cero e infinito. Esto lo hace muy adecuado para ser modelado matemáticamente. Sobre todo si tomamos su logaritmo, ya que en ese caso cualquier valor es posible. El modelo que consideraremos posteriormente será el de regresión logística.El modelo logístico de regresión puede usarse para determinar intervalos de confianza para la \\(\\):El modelo logístico de regresión puede usarse para determinar intervalos de confianza para la \\(\\):Si dichos intervalos contienen al valor \\(=1\\), puede rechazarse que el factor de riesgo (o el tratamiento) sea tal.Si dichos intervalos contienen al valor \\(=1\\), puede rechazarse que el factor de riesgo (o el tratamiento) sea tal.En otro caso decimos que aumenta o disminuye la oportunidad del evento en función de que el intervalo de confianza sea de valores mayores o menores que uno respectivamente.En otro caso decimos que aumenta o disminuye la oportunidad del evento en función de que el intervalo de confianza sea de valores mayores o menores que uno respectivamente.Cuando se evalúa la eficacia de una prueba diagnóstica es razonablemente simple conocer la sensibilidad y especificidad de la misma, pero los valores predictivos requieren del conocimiento de la prevalencia, que está siempre disponible. Si realizamos un estudio caso-control, donde la prevalencia de la enfermedad es desconocida, aunque podamos calcular índices predictivos, siempre podremos estimar la \\(\\). Si la enfermedad (el evento de interés) es rara, podemos considerarla como una aproximación del \\(RR\\), que tiene una interpretación muy natural.Cuando se evalúa la eficacia de una prueba diagnóstica es razonablemente simple conocer la sensibilidad y especificidad de la misma, pero los valores predictivos requieren del conocimiento de la prevalencia, que está siempre disponible. Si realizamos un estudio caso-control, donde la prevalencia de la enfermedad es desconocida, aunque podamos calcular índices predictivos, siempre podremos estimar la \\(\\). Si la enfermedad (el evento de interés) es rara, podemos considerarla como una aproximación del \\(RR\\), que tiene una interpretación muy natural.","code":""},{"path":"regresión-logística-binaria.html","id":"el-modelo-de-regresión-logística-binaria","chapter":"Capítulo 8 Regresión logística binaria","heading":"8.2 El modelo de regresión logística binaria","text":"Si tenemos una variable que describe una respuesta en forma de dos posibles eventos (vivir o , enfermar o ), y queremos estudiar el efecto que otras variables (independientes) tienen sobre ella (fumar, edad), el modelo de regresión logística binaria puede resultarnos de gran utilidad para:Dado los valores de las variables independientes, estimar la probabilidad de que se presente el evento de interés (por ejemplo, enfermar.)Dado los valores de las variables independientes, estimar la probabilidad de que se presente el evento de interés (por ejemplo, enfermar.)Podemos evaluar la influencia que cada variable independiente tiene sobre la respuesta, en forma de . Una mayor que uno indica aumento en la probabilidad del evento y menor que uno, implica disminución.Podemos evaluar la influencia que cada variable independiente tiene sobre la respuesta, en forma de . Una mayor que uno indica aumento en la probabilidad del evento y menor que uno, implica disminución.Para construir un modelo de regresión logística binarianecesitamos:Un conjunto de variables independientes o predictoras, muy en el estilo de la regresión lineal múltiple.Un conjunto de variables independientes o predictoras, muy en el estilo de la regresión lineal múltiple.Una variable respuesta dicotómica. Aquí se diferencia del modelo de regresión múltiple, donde la variable respuesta era numérica.Una variable respuesta dicotómica. Aquí se diferencia del modelo de regresión múltiple, donde la variable respuesta era numérica.","code":""},{"path":"regresión-logística-binaria.html","id":"codificación-de-las-variables","chapter":"Capítulo 8 Regresión logística binaria","heading":"8.2.1 Codificación de las variables","text":"Para simplificar la interpretación del análisis del modelo de regresión logística es conveniente llegar cierto convenio en la codificación de las variables. Realmente compensa seguir las siguientes recomendaciones:En la variable dependiente se codifica como 1 la ocurrencia del evento de interés y como 0 la ausencia.En la variable dependiente se codifica como 1 la ocurrencia del evento de interés y como 0 la ausencia.Las variables independientes pueden ser varias y cada una de un tipo diferente. Analicemos cada caso:Las variables independientes pueden ser varias y cada una de un tipo diferente. Analicemos cada caso:Caso dicotómico: Se codifica como 1 el caso que se cree favorece la ocurrencia del evento. Se codifica como 0 el caso contrario. Por ejemplo con 0 codificamos típicamente los individuos expuestos un posible factor de riesgo (casos de referencia, controles), y como 1 los expuestos.Caso dicotómico: Se codifica como 1 el caso que se cree favorece la ocurrencia del evento. Se codifica como 0 el caso contrario. Por ejemplo con 0 codificamos típicamente los individuos expuestos un posible factor de riesgo (casos de referencia, controles), y como 1 los expuestos.Caso categórico: Cuando la variable independiente puede tomar más de dos posibles valores podemos codificarlas usando variables indicadoras (dummy), como se hacía con el modelo de regresión lineal múltiple. Si estamos usando SPSS, el programa nos ayuda hacerlo sobre la marcha (hay que indicar el diálogo de regresión logística qué variables son categóricas). Es necesario destacar una modalidad que represente al caso de referencia, y al que le corresponde la codificación con todas las variables indicadoras puestas 0.Caso categórico: Cuando la variable independiente puede tomar más de dos posibles valores podemos codificarlas usando variables indicadoras (dummy), como se hacía con el modelo de regresión lineal múltiple. Si estamos usando SPSS, el programa nos ayuda hacerlo sobre la marcha (hay que indicar el diálogo de regresión logística qué variables son categóricas). Es necesario destacar una modalidad que represente al caso de referencia, y al que le corresponde la codificación con todas las variables indicadoras puestas 0.Caso de variable numérica: pueden darse dos situaciones:\n - Si creemos que por cada unidad que aumente la variable, la aumenta en un factor multiplicativo constante, podemos usar la variable tal cual en el modelo. Si tenemos dudas de que esto sea así, o sepamos ni siquiera lo que significa la frase anterior, mejor olvidamos esta posibilidad y consideramos la siguiente;\n - Si creemos que la variable numérica puede afectar la respuesta, pero tenemos muy claro de qué manera, podemos “categorizar” la variable. Esto consiste por ejemplo en estratificar la variable en valores pequeños, medianos y grandes. Los puntos de corte los podemos elegir nosotros manualmente, o usar cortes automáticos basados en que cada categoría tenga el mismo número de observaciones (usando percentiles). En SPSS la opción de menú “Transformar – Categorizador visual…” de SPSS nos puede ser de gran ayuda.Caso de variable numérica: pueden darse dos situaciones:\n - Si creemos que por cada unidad que aumente la variable, la aumenta en un factor multiplicativo constante, podemos usar la variable tal cual en el modelo. Si tenemos dudas de que esto sea así, o sepamos ni siquiera lo que significa la frase anterior, mejor olvidamos esta posibilidad y consideramos la siguiente;\n - Si creemos que la variable numérica puede afectar la respuesta, pero tenemos muy claro de qué manera, podemos “categorizar” la variable. Esto consiste por ejemplo en estratificar la variable en valores pequeños, medianos y grandes. Los puntos de corte los podemos elegir nosotros manualmente, o usar cortes automáticos basados en que cada categoría tenga el mismo número de observaciones (usando percentiles). En SPSS la opción de menú “Transformar – Categorizador visual…” de SPSS nos puede ser de gran ayuda.","code":""},{"path":"regresión-logística-binaria.html","id":"requisitos-y-limitaciones-1","chapter":"Capítulo 8 Regresión logística binaria","heading":"8.2.2 Requisitos y limitaciones","text":"Además de las mencionadas en cuanto los criterios para codificar la variables debemos tener en cuenta muchas otras cuestiones para confiar en la validez del modelo. De entre ellas destacamos:Los parámetros del modelo se calculan usando una estimación de máxima verosimilitud. Estas sólo son válidas cuando para cada combinación de variables independientes tenemos un número suficientemente alto de observaciones. Si los parámetros estimados en el modelo son anormalmente grandes, posiblemente esta condición sea violada. Tal vez se solucione el problema agrupando categorías (donde tenga sentido).Los parámetros del modelo se calculan usando una estimación de máxima verosimilitud. Estas sólo son válidas cuando para cada combinación de variables independientes tenemos un número suficientemente alto de observaciones. Si los parámetros estimados en el modelo son anormalmente grandes, posiblemente esta condición sea violada. Tal vez se solucione el problema agrupando categorías (donde tenga sentido).debemos introducir variables innecesarias. Ver el punto anterior.debemos introducir variables innecesarias. Ver el punto anterior.Ninguna variable relevante debe ser excluida. Si identificamos variables confusoras, tengámoslas en cuenta introduciéndolas en el modelo o estratificando el estudio en submuestras.Ninguna variable relevante debe ser excluida. Si identificamos variables confusoras, tengámoslas en cuenta introduciéndolas en el modelo o estratificando el estudio en submuestras.La colinealidad es un problema como ocurría en la regresión lineal múltiple. Si los errores típicos en la estimación de los coeficientes, o los intervalos de confianza son anormalmente grandes, es posible que esta situación se esté dando.La colinealidad es un problema como ocurría en la regresión lineal múltiple. Si los errores típicos en la estimación de los coeficientes, o los intervalos de confianza son anormalmente grandes, es posible que esta situación se esté dando.","code":""},{"path":"regresión-logística-binaria.html","id":"interpretación-del-modelo","chapter":"Capítulo 8 Regresión logística binaria","heading":"8.2.3 Interpretación del modelo","text":"El modelo de regresión logística puede escribirse como:\\[\\log(\\frac{p}{1-p})=b_0 + b_1 x_1 +b_2 x_2+\\dots\\]\ndonde \\(p\\) es la probabilidad (riesgo) de que ocurra el evento de interés, las variables independientes están representadas con la letra \\(x\\), y los coeficientes asociados cada variable con la letra $b4. Tal vez con esa expresión el modelo resulte muy elocuente, pero tras unas transformaciones, que nos mostramos para ahorrar espacio mostramos lo que resulta de mayor interés:\\[\np=\\frac{e^{suma}}{1+e^{suma}}, \\mbox{ siendo } suma=b_0 + b_1 x_1 +b_2 x_2+\\dots\n\\]Dado el valor de las variables independientes, podemos calcular directamente la estimación del riesgo de que ocurra el evento de interés:Dado el valor de las variables independientes, podemos calcular directamente la estimación del riesgo de que ocurra el evento de interés:La oportunidad (odds) para los individuos donde todos \\(x_i\\) vale cero es \\(e^{b_0}\\). En estudios caso-control, estos suelen ser los controles.La oportunidad (odds) para los individuos donde todos \\(x_i\\) vale cero es \\(e^{b_0}\\). En estudios caso-control, estos suelen ser los controles.Si nos fijamos en cualquier otro coeficiente del modelo, la cantidad \\(e*{b_i}\\) coincide con la del aumento del valor de \\(x_i\\) en una unidad con respecto aquellos individuos que presentan los valores de todas las demás variables iguales. Si hemos seguido el criterio de codificación recomendado, y la variable de la que hablamos es dicotómica, esto corresponde la del factor de riesgo \\(x_i\\). Si la variable es numérica como el número de bypass coronarios, estima la del factor de riesgo “tener un bypass más”.Si nos fijamos en cualquier otro coeficiente del modelo, la cantidad \\(e*{b_i}\\) coincide con la del aumento del valor de \\(x_i\\) en una unidad con respecto aquellos individuos que presentan los valores de todas las demás variables iguales. Si hemos seguido el criterio de codificación recomendado, y la variable de la que hablamos es dicotómica, esto corresponde la del factor de riesgo \\(x_i\\). Si la variable es numérica como el número de bypass coronarios, estima la del factor de riesgo “tener un bypass más”.Hay mucho que interpretar en una salida de ordenador en un cálculo de regresión logística. Aquí vamos mencionar sólo algunas de las que consideramos más interesantes en una primera aproximación:-Significación de cada coeficiente del modelo (basada en el estadístico de Wald): Ofrece el equivalente la significación de los coeficientes de regresión lineal múltiple. Si una variable independiente resulta significativa podemos considerar eliminarla del modelo (menos que esté confundida con otra variable independiente significativa, claro está).\nLa significación del estadístico de Wald para el coeficiente bi es la que corresponde contrastar la hipótesis nula de que éste vale cero. O lo que es lo mismo que la asociada, exp(bi)=1, es decir, que la variable xi es factor de riesgo y por ello podemos olvidarnos de ella menos que la encontremos confundida con otra que sí lo sea.\\(e^{b_i}=\\) estimada para la variable \\(x_i\\), donde \\(b_i\\) Aparecen en la columna “B” de la salida de SPSS. Más interesante aún son los intervalos de confianza para la \\(Exp(bi)=\\) (si hemos marcado la opción para que los calcule). Si contienen al valor uno, es señal de que la variable es de interés en el modelo. Estadísticamente es lo mismo que lo mencionado en el punto anterior, pero de este modo tiene más significado clínico.Para interpretar un modelo de regresión logística binaria, siga el enlace: https://www.bioestadistica.uma.es/analisis/logistica/El análisis con SPSS de la base de datos que descargaremos de allí (datos/regresionLogistica.sav)[datos/regresionLogistica.sav] lo haremos en el menú de SPSS: “Analizar - Regresión - Logística binaria”, colocando en el campo “Dependiente” la variable Enfermedad, y en las demás en el campo “Covariables”. Es conveniente pedir que se muestren los intervalos de confianza para Odds Ratios. Para ello en el botón “Opciones” marcamos “CI para exp(B)”. Los valores Exp(B) y sus intervalos de confianza en la tabla nos muestran las . También conviene marcar la casilla de cálculo de Bondad de ajuste de Hosmer-Lemeshow. Esta es una prueba que cuanto más alejado esté de la significación mejor indicador es de que el modelo de regresión logística es es inadecuado para los datos.","code":""},{"path":"taller-de-regresión-lineal.html","id":"taller-de-regresión-lineal","chapter":"Taller de regresión lineal","heading":"Taller de regresión lineal","text":"En los siguientes problemas vamos utilizar unas bases de datos simuladas, donde en cada base ocurre algo especial que debemos intentar visualizar y modelar de forma convenienteusando regresión lineal múltiple. En todos los casos suponemos que tenemos una base de datos de niños, donde se ha recogido el sexo con una variable que indica con el valor 1 las mujeres (0 para los Hombres), Edad y Talla. Nuestra intención es ver como el sexo y la edad explican la talla.","code":""},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-1-1","chapter":"Taller de regresión lineal","heading":"Ejemplo 1","text":"Descargue la base de datos\nregresion-030-1.sav y exploramos las primeras líneas:Antes de comenzar analizar los datos hacemos una representación gráfica del contenido de la base de datos:Los cuatro modelos de regresión respectivos son:Modelo 1: La talla sólo depende del sexo.Modelo 1: La talla sólo depende del sexo.Modelo 2: La talla sólo depende de la altura (mi favorito, por su simplicidad).Modelo 2: La talla sólo depende de la altura (mi favorito, por su simplicidad).Modelo 3 La talla depende de la altura, y hay un pequeño efecto debido al sexo. las dos líneas son paralelas, siendo la distancia vertical entre ambas el efecto estimado atribuible al sexo.Modelo 3 La talla depende de la altura, y hay un pequeño efecto debido al sexo. las dos líneas son paralelas, siendo la distancia vertical entre ambas el efecto estimado atribuible al sexo.Modelo 4 La talla depende de la altura para cada sexo, con rectas que tienen por qué ser paralelas. Se ajusta la recta dentro de cada grupo. Esto es equivalente considerar que hay un término de interacción que permite que cada recta tenga pendientes diferentes. Este modelo parece demasiado complicado.Modelo 4 La talla depende de la altura para cada sexo, con rectas que tienen por qué ser paralelas. Se ajusta la recta dentro de cada grupo. Esto es equivalente considerar que hay un término de interacción que permite que cada recta tenga pendientes diferentes. Este modelo parece demasiado complicado.El análisis de estos modelos lo encontramos en la siguiente tabla:El primer modelo, es el menos adecuado. Es el de menor \\(R^2\\), y atribuye un efecto significativo poramente al sexo, cuando en todos los demás modelos en que se incluye la edad, el sexo tiene efecto significativo. Lo que vemos es debido al efecto confusor de la edad, que ha sido tenido en cuenta.El segundo modelo es el que tomaría como más adecuado por su parsimonia. Es simple y parece una buena simplificación de las observaciones. La bondad de ajuste \\(R^2\\) es prácticamente tan buena como se puede conseguir, y tiene el menor AIC. La única variable de la que depende (edad) el modelo tiene un efecto estadísticamente significativo.El tercer modelo posee el mayor \\(R2\\) ajustado. Nos indica que la edad tiene efecto significativo en la talla, y que el sexo tiene efecto significativo en la talla. Si queremos estudiar qué efecto tiene la edad en la talla, descontando/ajustando/teniendo en cuenta el sexo, es el modelo utilizar.El cuarto modelo es demasiado complicado. Atendiendo al criterio de \\(R^2\\), parece el mejor, pero eso es lógico ya que contiene todas las variables explicativas y además su interacción. La reducción de \\(R^2\\) ajustado con respecto al tercer modelo, así como el aumento en AIC, y la falta de significación del término de interacción de Sexo con Edad, nos invitan abandonarlo. En caso de querer un modelo complejo, el tercero es lo suficientemente complejo.","code":"\ndf=read_sav(\"datos/regresion-030-1.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndf %>% head()  %>% knitr::kable(booktabs=T)\nggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()\nggplot(df, aes(x=Sexo, y=Talla))+geom_boxplot()\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()"},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-2-1","chapter":"Taller de regresión lineal","heading":"Ejemplo 2","text":"Descargue la base de datos\nregresion-200-1.sav, que contiene datos similares al anterior, pero ahora la muestra está formada por 200 individuos. ¿cree que el segundo modelo es el más adecuado?","code":"\ndf=read_sav(\"datos/regresion-200-1.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()"},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-3-1","chapter":"Taller de regresión lineal","heading":"Ejemplo 3","text":"Descargue la base de datos\nregresion-030-2.sav. En este caso tenemos niños que igualdad de edad suelen ser más altos que las niñas. En ambos sexos, se crece con la edad un ritmo similar. ¿Qué modelo cree que es más adecuado utilizar?","code":"\ndf=read_sav(\"datos/regresion-030-2.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE)%>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()"},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-4","chapter":"Taller de regresión lineal","heading":"Ejemplo 4","text":"Descargue la base de datos\nregresion-200-2.sav. Es como el anterior, pero con una muestra mucho mayor. Observe como con el aumento del tamaño de muestra se reducen los errores estándar/típicos de estimación de los parámetros, lo que puede ofrecer unos p más pequeños eventualmente, pero nada más cambia notablemente.","code":"\ndf=read_sav(\"datos/regresion-200-2.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()"},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-5","chapter":"Taller de regresión lineal","heading":"Ejemplo 5","text":"Descargue la base de datos\nregresion-020-3.sav. En este caso tenemos niños y niñas que hacia los 10 años tienen la misma talla, pero de ahí en adelante, los niños crecen un ritmo mayor que las niñas (aunque es dificil apreciar dicho efecto con muestras pequeñas) ¿Qué modelo cree que lo refleja mejor?","code":"\ndf=read_sav(\"datos/regresion-020-3.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()"},{"path":"taller-de-regresión-lineal.html","id":"ejemplo-6","chapter":"Taller de regresión lineal","heading":"Ejemplo 6","text":"Descargue la base de datos\nregresion-200-3.sav. Como el ejemplo anterior, pero usando muestras grandes, donde es más fácil apreciar las interacciones. ¿Que ve que haya cambiado con respecto al ejemplo anterior?","code":"\ndf=read_sav(\"datos/regresion-200-3.sav\", user_na=FALSE) %>% haven::as_factor() %>% \n  mutate(Sexo=Mujer) %>% select(Sexo,Edad,Talla)\ndfAumentado <- broom::augment(lm(Talla ~ Edad+Sexo,data=df))\nplot_SoloSexo=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE,formula = y ~ 1)+ggtitle(\"(1) Talla ~ Sexo\")+ guides(color=FALSE,shape=FALSE)\nplot_SoloEdad=ggplot(df, aes(x=Edad,y=Talla))+geom_point(aes(shape=Sexo,color=Sexo))+\n  geom_smooth(method = \"lm\",se=FALSE)+ggtitle(\"(2) Talla ~ Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_SexoEdad=ggplot(dfAumentado, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_line(aes(y = .fitted))+ggtitle(\"(3) Talla ~ Sexo+Edad\")+ guides(color=FALSE,shape=FALSE)\nplot_Interaccion=ggplot(df, aes(x=Edad,y=Talla,shape=Sexo,color=Sexo))+geom_point()+\n  geom_smooth(method = \"lm\",se=FALSE) +ggtitle(\"(4) Talla ~ Sexo+Edad+ Sexo:Edad\")+ guides(color=FALSE,shape=FALSE)\ngrid.arrange(plot_SoloSexo,plot_SoloEdad,plot_SexoEdad,plot_Interaccion)\nmodeloSexo <- df %>% lm(Talla ~ Sexo, data=.)\nmodeloEdad <- df %>% lm(Talla ~ Edad, data=.)\nmodeloSexoEdad <- df %>% lm(Talla ~ Sexo+Edad, data=.)\nmodeloInteraccion <- df %>% lm(Talla ~ Sexo*Edad, data=.)\ntab_model(modeloSexo, modeloEdad,show.se = TRUE,show.ci = FALSE,show.aic = TRUE) %>% .[[\"knitr\"]] %>% \n    asis_output()\ntab_model(modeloSexoEdad,modeloInteraccion,show.se = TRUE,show.ci = FALSE,show.aic = TRUE)%>% .[[\"knitr\"]] %>% \n    asis_output()"}]
